{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction This is for advanced data analysts ... "},"Databases/":{"url":"Databases/","title":"Databases","keywords":"","body":"Introduction mysql hive presto mongodb redis "},"Databases/Presto/":{"url":"Databases/Presto/","title":"Presto","keywords":"","body":"Presto 介绍了常用的Presto函数 "},"Databases/Presto/Presto简介&时间函数.html":{"url":"Databases/Presto/Presto简介&时间函数.html","title":"Presto简介&时间函数","keywords":"","body":"Presto简介 Presto是什么 Presto是由Facebook开发的分布式SQL 查询引擎，用来进行高速、实时的数据分析 Presto的产生是为了解决Hive的MapReduce模型太慢的问题 Presto是一个计算引擎，它不存储数据，通过丰富的Connector获取第三方的数据，并支持扩展 注意点：查询引擎，非数据库，不存储数据,获取第三方的数据 hive :（基于Hadoop的数据仓库工具）提供存储、查询 Presto优点 支持标准SQL，降低使用门槛 可以连接多种数据源，包括Hive、RDBMS（Mysql、Oracle、Tidb等）、Kafka、MongoDB、Redis等 一个低延迟高并发的内存计算引擎，相比Hive，执行效率要高很多 注意点：连接多种数据源，内存计算 Presto模型 查询示例：select \\ from hive.table1 a left join hive.table2 b on a.id = b.id* Catalog:就是数据源。Hive，Mysql是数据源，Hive 和Mysql都是数据源类型，可以连接多个Hive和多个Mysql。 Schema：相当于一个数据库实例，一个Schema包含多张数据表。 Table：数据表，与一般意义上的数据库表相同。 Presto日期时间函数 时区转换 格式: SELECT timestamp '2019-08-01 01:00 UTC' AT TIME ZONE 'America/Los_Angeles' UTC:格林威治时间/称世界统一时间、世界标准时间 比北京时间慢8个小时 ​ 示例:SELECT now() AT TIME ZONE 'America/Los_Angeles -- 比北京慢15小时北京时间 当前日期时间 函数 now()/ current_timestamp 返回timestamp with time zone** current_date：返回当前日期，只包含年月日 current_time: 返回time with time zone 示例:SELECT NOW() \"now\", current_timestamp \"current_timestamp\" , current_date \"current_date\", current_time \"current_time\" 注意: mysql:now()/sysdate(),curtime(),curdate() presto sql:current_date,current_time,current_timestamp函数都没有括号 时间格式转换 函数:cast(value AS type) → type 显式把value转换到type类型。 Presto SQL大的“特点”：完全不同的数据类型之间不能比较，相似的数据类型一般可以比较。 date与varchar完全不同,不可以比较 cast('2019-08-01' as date),date('2019-08-01' ) cast('2019-08-01 16:00:00.000' as TIMESTAMP) cast('2019-08-01 16:00:00.000' as date) 报错 需要截取 注意:长转短需要截取,短转长会自动补齐 示例:select current_date > cast('2019-07-31' as date) ; ​ select current_date > date'2019-07-31' ; ​ select '2019-08-01' > '2019-07-31' ; ​ select substr(lp.adjust_start_time,1,10) ,lp.adjust_start_time ​ from table lp ​ where substr(lp.time,1,10) = '2019-08-01' limit 1; 截取函数 ​ date_trunc(unit, x) 返回x截取到单位unit之后的值 函数 支持如下单位： 单位 Example Truncated Value second 2001-08-22 03:04:05.000 minute 2001-08-22 03:04:00.000 hour 2001-08-22 03:00:00.000 day 2001-08-22 00:00:00.000 week 2001-08-20 00:00:00.000 month 2001-08-01 00:00:00.000 quarter 2001-07-01 00:00:00.000 year 2001-01-01 00:00:00.000 示例:select date_trunc('hour',current_timestamp); ​ select date_trunc('week',current_timestamp); ​ select date_trunc('month',current_timestamp); ​ select date_trunc('quarter',current_timestamp); 间隔函数 date_add(unit, value, timestamp)→ [same as input] 在timestamp的基础上加上value个unit。如果想要执行相减的操作，可以通过将value赋值为负数来完成。 函数支持如下所列的间隔单位： Unit Description second Seconds minute Minutes hour Hours day Days week Weeks month Months quarter Quarters of a year year Years 示例: select date_add('day',1,now()); ​ select date_add('hour',-1,now()); ​ select date_add('quarter',1,now()); -- 增加一个季度即增加三个月 时间差函数 date_diff(unit, timestamp1, timestamp2) 返回 (timestamp2 - timestamp1) 之后的值，该值的表示单位是unit. 注意:unit加引号 ,mysql 中 DATEDIFF(date1,date2) 是date1-date2 示例: select date_diff('hour', TIMESTAMP'2018-08-08 08:08:08', TIMESTAMP'2018-08-08 00:00:00') ; ​ select date_diff('quarter', TIMESTAMP'2019-08-08 08:08:08', TIMESTAMP'2019-12-02 06:00:00.000') ; 日期格式化函数 date_format(timestamp, format) → varchar 使用format指定的格式，将timestamp格式化成字符串 支持类型单位 注意:%D %U %u %V %X %w 暂不支持* 分类符 说明 %a Abbreviated weekday name (Sun .. Sat) %b Abbreviated month name (Jan .. Dec) %c Month, numeric (0 .. 12) %D Day of the month with English suffix (0th, 1st, 2nd, 3rd, …) %d Day of the month, numeric (00 .. 31) %e Day of the month, numeric (0 .. 31) %f Fraction of second (6 digits for printing: 000000 .. 999000; 1 - 9 digits for parsing: 0 .. 999999999) %H Hour (00 .. 23) %h Hour (01 .. 12) %I Hour (01 .. 12) %i Minutes, numeric (00 .. 59) %j Day of year (001 .. 366) %k Hour (0 .. 23) %l Hour (1 .. 12) %M Month name (January .. December) %m Month, numeric (00 .. 12) %p AM or PM %r Time, 12-hour (hh:mm:ss followed by AM or PM) %S Seconds (00 .. 59) %s Seconds (00 .. 59) %T Time, 24-hour (hh:mm:ss) %U Week (00 .. 53), where Sunday is the first day of the week %u Week (00 .. 53), where Monday is the first day of the week %V Week (01 .. 53), where Sunday is the first day of the week; used with %X %v Week (01 .. 53), where Monday is the first day of the week; used with %x %W Weekday name (Sunday .. Saturday) %w Day of the week (0 .. 6), where Sunday is the first day of the week %X Year for the week where Sunday is the first day of the week, numeric, four digits; used with %V %x Year for the week, where Monday is the first day of the week, numeric, four digits; used with %v %Y Year, numeric, four digits %y Year, numeric (two digits) %% A literal % character %x x, for any x not listed above 示例:SELECT date_format(current_timestamp,'%Y-%m-%d %W %H:%i:%s') date_parse(string, format) → timestamp 按照format指定的格式，将字符串string解析成timestamp 注意:两边形式要一样 date_parse('2019-08-01 ','%Y-%m-%d %H:%i:%s') 会报错 示例: SELECT date_parse('2019-08-01 08:08:08','%Y-%m-%d %H:%i:%s'); 抽取函数 extract(field FROM x)→ bigint(整数) 从x中返回域field 抽取函数支持的数据类型取决于需要抽取的域。大多数域都支持日期和时间类型。 可以使用抽取函数来抽取如下域： Field Description YEAR year() QUARTER quarter() MONTH month() WEEK week() DAY day() DAY_OF_MONTH day() DAY_OF_WEEK day_of_week() DOW day_of_week() DAY_OF_YEAR day_of_year() DOY day_of_year() YEAR_OF_WEEK year_of_week() YOW year_of_week() HOUR hour() MINUTE minute() SECOND second() TIMEZONE_HOUR timezone_hour() :表示时区偏移小时量 TIMEZONE_MINUTE timezone_minute() 示例: select extract(year FROM current_timestamp) ,extract(month from DATE'2019-08-01'),extract(DAY FROM now()), extract(WEEK from now()),extract(doy FROM now()); ​ select extract(TIMEZONE_HOUR FROM now()) \"时区偏移小时量\"; 便利的抽取函数: 格式 day(x) → bigint 返回指定日期在当月的天数 day_of_month(x) → bigint day(x)的另一种表述 day_of_week(x) → bigint 返回指定日期对应的星期值，值范围从1 (星期一) 到 7 (星期天). day_of_year(x) → bigint 返回指定日期对应一年中的第几天，值范围从1到 366. dow(x) → bigint day_of_week()的另一种表达 doy(x) → bigint day_of_year()的另一种表达 hour(x) → bigint 返回指定日期对应的小时，值范围从1到 23 minute(x) → bigint 返回指定日期对应的分钟 month(x) → bigint 返回指定日期对应的月份 quarter(x) → bigint 返回指定日期对应的季度，值范围从1到 4 second(x) → bigint 返回指定日期对应的秒 timezone_hour(timestamp) → bigint 返回从指定时间戳对应时区偏移的小时数 timezone_minute(timestamp) → bigint 返回从指定时间戳对应时区偏移的分钟数 week(x) → bigint 返回指定日期对应一年中的ISO week，值范围从1到 53 week_of_year(x) → bigint week的另一种表述 year(x) → bigint 返回指定日期对应的年份 year_of_week(x) → bigint 返回指定日期对应的ISO week的年份 yow(x) → bigint year_of_week()的另一种表达 示例:select year(now()) ,month(now()),day(now()),doy(now()),quarter(now()),week(now()); 补充: sum() over() sum() over(PARTITION BY columnA order by columnB ) :类似于Python中 cumsum(),实现累计和功能 官方文档 http://prestodb.jd.com/docs/current/index.html* "},"Databases/Presto/Presto常用函数.html":{"url":"Databases/Presto/Presto常用函数.html","title":"Presto常用函数","keywords":"","body":"1.字符串长度计算函数：length 语法: length(string A) select length('iteblog') 2.字符串反转函数：reverse 语法: reverse(string A) 返回值: string 说明：返回字符串A的反转结果 select reverse('iteblog') 3.字符串连接函数：concat 语法: concat(string A, string B…) 返回值: string 说明：返回输入字符串连接后的结果，支持任意个输入字符串 select concat('www','iteblog','com') 使用||运算符实现字符串的拼接(快) select 'www' || 'iteblog' || 'com' 4.字符串截取函数：substr,substring 语法: substr(string A, int start),substring(string A, int start) 返回值: string 说明：返回字符串A从start位置到结尾的字符串 select substr('iteblog',3) select substr('iteblog',3,2) select substr('iteblog',-3,2) select substring('iteblog',3) select substring('iteblog',3,2) select substring('iteblog',-3,2) 5.replace(string, search) → varchar 删除字符串 string 中的所有子串 search 。 select replace('itebtlog', 't') replace(string, search, replace) → varchar 将字符串 string 中所有子串 search 替换为 replace。 select replace('itebtlog', 't','k') 6.strpos(string, substring) → bigint 返回字符串中子字符串的第一次出现的起始位置。位置以 1 开始 ，如果未找到则返回 0 。 select strpos('titebtlog', 't') select strpos('itebtlog', 'p') 7.regexp_replace:将原字符中的指定字符替换 select regexp_replace('aabb','b','c') 8.regexp_extract：通过下标返回正则表达式指定的部分 select regexp_extract('123abc456','([0-9]*)([a-z]*)([0-9]*)',3) 9.coalesce:返回参数列表中第一个非null值(非空值) select coalesce(null,'a','b') select coalesce(null,null,null) 将null值替换 select student_intention_id, student_no, name, coalesce(name,'小花') from view_student where name is null limit 10 10.条件函数:if select if(1+1=2,'a','b') 11.窗口函数 first_value(x):返回窗口内的第一个value，一般用法是窗口内数值排序，获取最大值。 last_value(x) 含义和first value相反。 sum() select lp.student_id, cast(substr(lp.adjust_start_time,1,19) as timestamp) adjust, teacher_id, first_value(teacher_id) OVER (PARTITION BY student_id ORDER BY cast(substr(lp.adjust_start_time,1,19) as timestamp) ), first_value(lp.adjust_start_time) OVER (PARTITION BY student_id ORDER BY cast(substr(lp.adjust_start_time,1,19) as timestamp) ) from lesson_plan lp where student_id in (1537357,1537006,1537362,1537336,1537025) 12.数组函数 array_agg (key)是一个聚合函数，表示把key这一列的所有内容变成一个数组返回。 array_distinct：返回去重后的数组 array_max(array) -> E 返回数组中最大的元素 array_min(array) -> E 返回数组中最小的元素 array_position(x, element) -> bigint 返回element在数组中的位置 contains(x, element) -> boolean 判断element是否在数组x中 array_remove(x, element) -> x 移除数组x中的所有element元素 cardinality(x) -> bigint 返回数组的元素个数 select lp.student_id, -- array_agg(sj.subject_name), -- array_agg(distinct sj.subject_name), -- array_distinct(array_agg(subject_name)) -- array_agg(distinct sj.subject_id), -- array_max(array_agg(distinct sj.subject_id)) -- array_position(array_agg(distinct sj.subject_name),'体验课'), -- contains(array_agg(distinct sj.subject_name),'体验课'), -- array_remove(array_agg(distinct sj.subject_name),'体验课') -- cardinality(array_agg(distinct sj.subject_name)) --array_join(array_agg(distinct subject_name), '-') subject_name from lesson_plan lp left join subject sj on sj.subject_id = lp.subject_id where student_id in (1537357,1537006,1537362,1537336,1537025) group by lp.student_id 13.lambda函数 lambda表达式的书写形式为-> (1)filter(array, function) -> array array中的每一个元素经过function过滤，返回都为true的元素 (2)transform(array, function) → ARRAY 对数组中的每个元素，依次调用function，生成新的结果U。 select lp.student_id, array_agg(distinct lp.lesson_type), filter(array_agg(distinct lp.lesson_type), x -> x=3), array_agg(distinct subject_name), transform(array_agg(distinct subject_name), x -> substr(x,1,2)) from lesson_plan lp left join subject sj on sj.subject_id = lp.subject_id where student_id in (1537357,1537006,1537362,1537336,1537025) group by lp.student_id "},"Databases/Presto/Presto数组&字符串函数.html":{"url":"Databases/Presto/Presto数组&字符串函数.html","title":"Presto数组&字符串函数","keywords":"","body":"数组函数 element_at(array,index) 返回数组中索引index对应的元素(index可以为负数) array_position(array, element) 返回element在数组中的位置(bigint) array_agg() 聚合函数，将一列所有内容变成一个数组返回。 array_join(array,delimiter,null_value_replacement) 将数组中的所有元素以delimiter指定的分隔符连接起来，null值用null_value_replacement代替，返回一个字符串 filter(array,function) 将函数function中返回true的元素来构造新数组 SELECT filter(ARRAY [], x -> true); -- [] SELECT filter(ARRAY [5, -6, NULL, 7], x -> x > 0); -- [5, 7] SELECT filter(ARRAY [5, NULL, 7, NULL], x -> x IS NOT NULL); -- [5, 7] 字符串函数 split(string，delimiter) 以delimiter分隔符拆分字符串,返回一个数组。 split_part （string，delimiter，index） 以delimiter分隔符拆分字符串,返回索引对应的字符串.字段索引以1开头(不可为负数).如果索引大于字段数，则返回null. 类似于mysql中substring_index函数(index>0). select split_part('北京,上海,广州',',',1) -- 北京 strpos(string，substring) 返回子字符串在字符串中第一次出现的位置。从1开始。如果未找到，则返回0。 类似于mysql中的locate函数,但locate是子字符串在前. select strpos('上海销售分公司四中心二区二部精英学院','精英学院') -- 15 substr(string,start) 从start位置开始返回字符串的其余部分。位置从1开始。如果start为负，则起始位置代表从字符串的末尾开始倒数 select substr('数学,英语,', -1) -- , select substr('数学,英语,', -1) -- 英语, substr(string, start, length) 从start位置开始返回长度为length的字符串的子串。位置从1开始。如果start为负，则起始位置代表从字符串的末尾开始倒数 组合使用 array_join(array_agg(),delimiter,null_value_replacement) 类似于mysql中的group_concat函数. select tcp.contract_id,array_join(array_agg(pay_method_new),',') from table1 tcp left join table2 tc on tc.contract_id = tcp.contract_id where tcp.contract_id = 'X20011907000631' and tcp.pay_status in (2,4) and tc.status not in (7,8) group by tcp.contract_id array_join(filter(array_agg(conditon),function),delimiter) select array_join(filter( array_agg(case when is_success_extend = 0 then tbk.extend_subject_name else null end),x -> x is not null ),',') 当需要加判断条件时,如果不加filter函数,用以下写法,字段最后可能会出现逗号 select array_join( array_agg(case when is_success_extend = 0 then tbk.extend_subject_name end), ',') extend_fail_subject_name element_at(split(string,delimiter),index) 类似于mysql中的substring_index函数. select element_at(split('北京,上海,广州',','),-1) -- 广州 select element_at(split('北京,上海,广州',','),2) -- 上海 select element_at(split('客户关系部,课程顾问部,江苏分公司,江苏销售一中心,江苏销售一中心一区,江苏销售一中心一区二部',','),-1) 去掉字符串末尾的逗号 select (case when substr('数学,英语,', -1) = ',' then substr('数学,英语,',1,length('数学,英语,')-1) end) -- 数学,英语 "},"DataAnalystHelper/":{"url":"DataAnalystHelper/","title":"DataAnalystHelper","keywords":"","body":"Introduction 本项目从BI层面提供数据服务支持方面, 封装了常用方法 使数据分析师在日常工作中快速搭建数据服务来专注业务逻辑 代码主要对以下几大点做封装 数据库连接查询(Mysql , Hive) 数据表生成(Mysql中间表) DataFrame常用方法二次封装(时间函数, 输出Excel, merge...) 输出内容相关(html模板渲染, 钉钉自定义机器人) ...... 具体代码请前往 https://github.com/advanceddataanalysts/AdvancedPython "},"PythonShare/":{"url":"PythonShare/","title":"PythonShare","keywords":"","body":"PythonShare "},"PythonShare/PythonTips/":{"url":"PythonShare/PythonTips/","title":"PythonTips","keywords":"","body":"PythonTips "},"PythonShare/PythonTips/动态执行代码.html":{"url":"PythonShare/PythonTips/动态执行代码.html","title":"动态执行代码","keywords":"","body":"Python动态导入模块&动态执行Python代码 1. 动态导入模块方法: ## 栗子:在lib下定义模块along_pack class FirstImport(object): def __init__(self, name=\"along\"): self.name = name def start(self): return \"你好呀 !\" + self.name 直接使用__import__模块 lib = __import__('lib.along_pack') c = lib.along_pack.FirstImport() print(c.start()) ## 你好呀 !along 使用import importlib里的 importlib.import_module方法 import importlib a = importlib.import_module('lib.along_pack') c = a.FirstImport(name='strong') print(c.start()) ## 你好呀 !strong getattr(obj,name,val):返回对象obj中名为name的属性值，如果没有该属性，且给定了val参数，则返回val hasattr(obj,name):如果对象obj中存在名为name的属性，则返回True delattr(obj,name):删除obj中名为name的属性 setattr(obj,name,val):将对象obj中名为name的属性值设置为val，如果不存在该属性，则进行创建 两种方法的区别: 方法2 代码更简洁,并且支持reload( ) 1 reload()重新加载，一般用于原模块有变化等特殊情况。 2 reload()之前该模块必须已经使用import导入模块。 3 重新加载模块，但原来已经使用的实例还是会使用旧的模块，而新生产的实例会使用新的模块，reload后还是用原来的内存地址。 2. 动态执行python代码 exec : 把传入的字符串当代码执行,不返回结果 a = 1 exec (\"a=2\") ##无结果输出 print(a) ## 2 b = \"pandas\" print(exec(\"import %s as pd\"%b)) ##None pd.read_excel(r'E:\\Desktop\\along_test\\along_hetong_test.xlsx') eval : 把传入的字串串当成表达式执行,返回结果 a = [1,2] b = ['along','strong'] a_dic = dict(zip(a,b)) print(a_dic) ## {1: 'along', 2: 'strong'} cc = \"a_dic[3] ='lp'\" eval(cc) ##报错 exec(cc) ##print(a_dict) {1: 'along', 2: 'strong', 3: 'lp'} ##两个都可以解析字符串中的字符当成关键字参数进行运算 dd = 'a+b' print(dd) ## 'a+b' eval(dd) ##[1, 2, 'along', 'strong'] exec(dd) ##print(dd) 'a+b' "},"PythonShare/PythonTips/正则替换非法字符.html":{"url":"PythonShare/PythonTips/正则替换非法字符.html","title":"正则替换非法字符","keywords":"","body":"正则替换非法字符 背景 : 在沟通相关的数据中, 比如沟通内容,这些字段为销售或班主任手动输入填写, 有时候会有一些特殊字符,比如 ︶︹︺ 解决方案 : 在window下一般使用GB18030编码也能解决, 但是部署到线上使用Linux执行脚本的时候会出现导出数据编码失败的问题 报错为 IllegalCharacterError() 普通的特殊字符直接按照一下处理 就可以把这些字符转换为空 x = '特殊字符巴拉巴拉' ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]') x = ILLEGAL_CHARACTERS_RE.sub(r'', x) 但是总有些你不知道的什么鬼符号出现,就连上面的编码都找不到,这个时候就要把这条记录找到然后把它的编码转化成' ' 做法非常暴力: ## 我得到的数据为df, 有问题的那一列为 df['最后沟通内容'] ## 因为是转换成excel那一步的时候出了问题, 所以就在那里打印检查 for i in range(df.shape[0]): if i "},"PythonShare/Dictionary.html":{"url":"PythonShare/Dictionary.html","title":"Dictionary","keywords":"","body":"Dictionary Q 构建字典时 创建字典引用其他字典时,不知道key是否存在,如果key存在则去重 pairs = {'apple':2.5, 'orange':2.2, 'banana':2.6, 'tuberose':999999999} pairs1 = {'apple':2.5, 'orange':2.2, 'banana':2.6, 'tuberose':999998, 'along':'refuse'} ## 方法一: dic = {} def add_dic(pairs,dic): for key,value in pairs.items(): if key not in dic: dic[key] = [] dic[key].append(value) dic[key] = list(set(dic[key])) return dic add_dic(pairs=pairs,dic=dic) add_dic(pairs=pairs1,dic=dic) ## 方法二: ## 代码结构更清晰 from collections import defaultdict s_dict = defaultdict(set) def s_add_dic(pairs,dic): for key,value in pairs.items(): s_dict[key].add(value) return dic s_add_dic(pairs=pairs,dic=s_dict) s_add_dic(pairs=pairs1,dic=s_dict) Q 构建字典时 保证字典有序 ## 方法一: ## python3.6之后字典会按照构建顺序自动排序 dic = {} dic['a'] = 1 dic['b'] = 2 dic['c'] = 3 dic['d'] = 4 print(dic) ## {'a': 1, 'c': 3, 'b': 2, 'd': 4} ## 方法二: ## 必按照添加顺序排序 w+s ## OrderedDict内部维护了双向链表,根据加入顺序排列键的位置,但是大小是普通字典的2倍大 from collections import OrderedDict s_dic = OrderedDict() s_dic['a'] = 1 s_dic['b'] = 2 s_dic['c'] = 3 s_dic['d'] = 4 print(dict(s_dic)) ## {'a': 1, 'b': 2, 'c': 3, 'd': 4} Q 构建字典时 从已有的字典中提取子集 ## 字典推导式 pairs = {'apple':2.5, 'orange':2.2, 'banana':2.6, 'tuberose':999999999} dic = {key:value for key,value in pairs.items() if key in ['apple','orange']} print(dic) Q 构建字典时 字典映射 pairs = {'apple':2.5, 'orange':2.2, 'banana':2.6, 'tuberose':999999999} pairs1 = {'apple':2.5, 'orange':2.2, 'banana':2.6, 'tuberose':999998, 'along':'refuse'} ## 方法一: ## 构建新的字典对象/在原始字典上修改 new_pairs = dict(pairs1) new_pairs.update(pairs) print(new_pairs) ## 方法二: ## ChainMap使用原始字典映射,更改原始字典同步反应到映射对象上 from collections import ChainMap cm_pairs = ChainMap(pairs,pairs1) print({key:value for key,value in cm_pairs.items()}) ## 对ChaipMap对象的修改只会反映到第一个dict对象上, 反之亦然, 对字典的修改只有第一个的修改会反映到ChaipMap对象上 cm_pairs['tuberose'] = 99999999999999999999 print(cm_pairs,'\\n',pairs,'\\n',pairs1) pairs1['tuberose'] = 9999999888888888999999999999 print({key:value for key,value in cm_pairs.items()}) Q 对字典进行计算 将字典的key替换, value不变 pairs = {'apple':2.5, 'orange':2.2, 'banana':2.6, 'tuberose':999999999} ## 方法一: pairs['new_apple'] = pairs.pop('apple') ## 方法二: pairs.update({'new_apple':pairs.pop(\"apple\")}) Q 对字典进行计算 求value最值对应的key:value对 prices = {'A':2.22,'B':9.06,'C':0.06,'D':9.06,'E':0.06} print(max(prices)) ## 'E' ## 多条记录value相等时 按照第二个参数大小排序 print(max(zip(prices.values(),prices.keys()))) ## (9.06, 'D') ## 多条记录value相等时 取第一次出现的键值对 print(max(prices, key=lambda k: prices[k]), prices[max(prices, key=lambda k: prices[k])]) ## B 9.06 Q 对字典进行计算 求两个/多个字典的相同点 a_dict = {'x':1, 'y':2, 'z':3} b_dict = {'x':1, 'y':2, 'w':3} c_dict = {'x':1} print(a_dict.keys() & b_dict.keys()) ##{'x', 'y'} print(a_dict.keys() & b_dict.keys() & c_dict.keys()) ##{'x'} print(a_dict.items() & b_dict.items()) ## {('x', 1), ('y', 2)} c = {key:a_dict[key] for key in a_dict.keys() - {'z','w'}} print(c) ##{'x': 1, 'y': 2} Q 对字典进行计算 通过公共键对字典列表排序 rows = [{'fname':11, 'lname':2, 'uid':1003}, {'fname':111, 'lname':22, 'uid':1004}, {'fname':111, 'lname':222, 'uid':1002}, {'fname':11, 'lname':222, 'uid':1003},] ## 方法一: sorted(rows, key=lambda x:x['uid']) sorted(rows, key=lambda x:(x['fname'],x['lname'])) ## 方法二: ## 运行速度更快,性能好(max,min也同样支持) from operator import itemgetter sorted(rows, key=itemgetter('uid'), reverse=True) sorted(rows, key=itemgetter('fname','lname'),reverse=False) ## 方法三: ## 转成df,排序结束再用df.to_dict()/df.to_json()转 import pandas as pd data = pd.DataFrame(rows) data.sort_values(by=['uid'],inplace=True) data.to_dict(orient='records') Q 对字典进行计算 根据键对数据进行分组 rows = [{'address':'zhongguancun', 'date':'07/01/2018'}, {'address':'putuo', 'date':'07/02/2018'}, {'address':'wangfujing', 'date':'07/01/2018'}, {'address':'jingan', 'date':'07/04/2018'},] ## 方法一: ## sort+itemgetter+groupby from operator import itemgetter from itertools import groupby rows.sort(key=itemgetter('date')) ##分组前需要先排序,因为groupby()只能检查连续的项 for key, value in groupby(rows, key=itemgetter('date')): print(key) for i in value: print(' ',i) ## 方法二: ## 转成df,groupby之后再转成字典 import pandas as pd data = pd.DataFrame(rows) for key,value in data.groupby(by=['date']): print(key,list(value.T.to_dict().values())) "},"PythonShare/DataStructure.html":{"url":"PythonShare/DataStructure.html","title":"DataStructure","keywords":"","body":"DataStructure * 解压 解压序列赋值给多个变量(作用可迭代对象) p = (4, 5) x, y = p data = [ 'ACME', 50, 91.1, (2012, 12, 21) ] name, shares, price, date = data name, shares, price, (year, mon, day) = data s = 'Hello' a, b, c, d, e = s # 变量个数和序列元素的个数不匹配，会产生一个异常 p = (4, 5) x, y, z = p # 部分解压 data = [ 'ACME', 50, 91.1, (2012, 12, 21) ] _, shares, price, _=data 解压可迭代对象赋值给多个变量 record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212') record = ('Dave', 'dave@example.com') record = ('Dave', 'dave@example.com','773-555-1212', '847-555-1212','773-555-1213', '847-555-1213') name, email, *phone_numbers = record phone_numbers #星号表达式也能用在列表的开始部分 *trailing, current = [10, 8, 7, 1, 9, 5, 10, 3] trailing current trailing, *current = [10, 8, 7, 1, 9, 5, 10, 3] trailing current line = 'nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false' uname, *fields, homedir, sh = line.split(':') 双向队列 deque:double-ended queue #双端队列是指允许两端都可以进行入队和出队操作的队列，其元素的逻辑结构仍是线性结构。 #将队列的两端分别称为前端和后端，两端都可以入队和出队 栈:栈是一种后进先出的数据结构，我们可以借助list来实现栈 stack = [] stack.append(item1) # 入栈 stack.pop() # 出栈，返回结果是出栈元素 peak = stack[-1] # 返回栈顶元素 队列:队列是一种先进先出的数据结构，我们可以借助list来实现队列 queue = [] queue.append(item) # 入队 queue.pop(0) # 出队，返回结果是出队元素 在队列两端插入或删除元素时间复杂度都是 O(1) ， 而在列表的开头插入或删除元素的时间复杂度为 O(N) 。 from collections import deque q = deque(maxlen=3) q.append(1) q.append(2) q.append(3) q.append(4) q.append(5) #q.appendleft(6) ###默认从右边 q.clear() #清空队列 q.count(n) #在队列中统计元素的个数，n表示统计的元素 q.extend(n) #从右边扩展队列，n表示扩展的队列 q.extendleft(n) #从左边扩展队列，n表示扩展的队列 q.pop() #队尾元素删去 q.popleft() #队头元素删去 Heap & Heapq 查找最大或最小的 N 个元素 堆： 1.建立在完全二叉树的基础上(若设二叉树的深度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数， 第 h 层所有的结点都连续集中在最左边，这就是完全二叉树。) 2.排序算法的一种，也是稳定效率最高的一种 3.可用于实现STL中的优先队列(priority_queue) STL是Standard Template Library的简称，中文名标准模板库，惠普实验室开发的一系列软件的统称。 它是由Alexander Stepanov、Meng Lee和David R Musser在惠普实验室工作时所开发出来的 **优先队列**：一种特殊的队列，队列中元素出栈的顺序是按照元素的优先权大小，而不是元素入队的先后顺序 4.两类： a.最大堆： ①根的值大于左右子树的值 ②子树也是最大堆 b.最小堆： ①根的值小于左右子树的值 ②子树也是最小堆 该模块提供了堆排序算法的实现,heapq有两种方式创建堆， 一种是使用一个空列表， 然后使用heapq.heappush()函数把值加入堆中，另外一种就是使用heap.heapify(list)转换列表成为堆结构 ##插入 import heapq #载入heap库，heap指的是最小堆 nums = [2, 3, 5, 1, 54, 23, 132] heap = [] for num in nums: heapq.heappush(heap, num) # 加入堆 print(heap[0]) # 如果只是想获取最小值而不是弹出，使用heap[0] print([heapq.heappop(heap) for _ in range(len(nums))]) # 堆排序结果 heap= [] nums = [2, 3, 5, 1, 54, 23, 132] heapq.heapify(nums) print(nums) print([heapq.heappop(heap) for _ in range(len(nums))]) import math from io import StringIO def show_tree(tree, total_width=36, fill=' '): \"\"\"Pretty-print a tree.\"\"\" output = StringIO() last_row = -1 for i, n in enumerate(tree): if i: row = int(math.floor(math.log(i + 1, 2))) else: row = 0 if row != last_row: output.write('\\n') columns = 2 ** row col_width = int(math.floor(total_width / columns)) output.write(str(n).center(col_width, fill)) last_row = row print(output.getvalue()) print('-' * total_width) print() data = [2, 3, 5, 1, 54, 23, 132] heap = [] for n in data: print('add {:>3}:'.format(n)) heapq.heappush(heap, n) show_tree(heap) print(heap) ##删除 #heapq.heappop() data = [2, 3, 5, 1, 54, 23, 132] heapq.heapify(data) heap = [] while data: i=heapq.heappop(data) print('pop %3d:' % i) show_tree(data) heap.append(i) print(heap) import heapq nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2] # print(heapq.nlargest(3, nums)) # Prints [42, 37, 23] # print(heapq.nsmallest(3, nums)) # portfolio = [ {'name': 'IBM', 'shares': 100, 'price': 91.1}, {'name': 'AAPL', 'shares': 50, 'price': 543.22}, {'name': 'FB', 'shares': 200, 'price': 21.09}, {'name': 'HPQ', 'shares': 35, 'price': 31.75}, {'name': 'YHOO', 'shares': 45, 'price': 16.35}, {'name': 'ACME', 'shares': 75, 'price': 115.65} ] cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price']) expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price']) ###最小堆转最大堆 只用对元素取负，按照最小堆的方式存储，取出来的值只要再取负，就是最大值了。即push(e)改为push(-e)，pop(e)为-pop(e) data = [2, 3, 5, 1, 54, 23, 132] heap = [] for n in data: print('add {:>3}:'.format(n)) heapq.heappush(heap, n) show_tree(heap) print(heap) heap1 = [] while heap: i=-1 * heapq.heappop(heap) print('pop %3d:' % i) show_tree(data) heap1.append(i) print(heap1) ####最大堆 实现一个优先级队列 import heapq class PriorityQueue: def __init__(self): self._queue = [] self._index = 0 def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1] class Item: def __init__(self, name): self.name = name def __repr__(self): return 'Item({!r})'.format(self.name) q = PriorityQueue() q.push(Item('foo'), 1) q.push(Item('bar'), 5) q.push(Item('spam'), 4) q.push(Item('grok'), 1) q.pop() q.pop() q.pop() q.pop() "},"PythonShare/Regex.html":{"url":"PythonShare/Regex.html","title":"Regex","keywords":"","body":"Regex 模式匹配: 模式匹配是数据结构中字符串的一种基本运算, 给定一个子串, 要求在某个字符串中找出与该子串相同的所有子串 Tips: 正则表达式使用反斜杠（'\\'）来表示特殊形式，或者把特殊字符转义成普通字符. 而反斜杠在普通的 Python 字符串里也有相同的作用，所以就产生了冲突. 比如说，要匹配一个字面上的反斜杠，正则表达式模式不得不写成 '\\\\\\\\'，因为正则表达式里匹配一个反斜杠必须是 \\\\ ，而每个反斜杠在普通的 Python 字符串里都要写成 \\\\ . 解决办法是对于正则表达式样式使用 Python 的原始字符串表示法: 在带有 'r' 前缀的字符串字面值中，反斜杠不必做任何特殊处理. 因此 r\"\\n\" 表示包含 '\\' 和 'n' 两个字符的字符串，而 \"\\n\" 则表示只包含一个换行符的字符串. 样式在 Python 代码中通常都会使用这种原始字符串表示法来表示. import re ## 不带'r'前缀 a = '''aaa\\\\bbbb''' re.split('\\\\\\\\',a) # ['aaa', 'bbbb'] ## 带'r'前缀 re.split(r'\\\\',a) # ['aaa', 'bbbb'] 正则表达式语法 用来检查某个字符串是否跟给定的正则表达式匹配 常用特殊字符及含义 . 匹配除了换行的任意字符 如果指定了标签 DOTALL (re.S, re.DOTALL)，它将匹配包括换行符的任意字符 ^ 匹配字符串的开头 $ 匹配字符串尾或者换行符的前一个字符 * 对它前面的正则式匹配0到任意次重复， 尽量多的匹配字符串 + 对它前面的正则式匹配1到任意次重复 ? 对它前面的正则式匹配0到1次重复 *?, +?, ?? '*', '+'和 '?' 修饰符都是贪婪的, 它们在字符串进行尽可能多的匹配 在修饰符之后添加 ? 将使样式以 非贪婪 方式进行匹配 , 尽量少的字符将会被匹配 {m-n} 对其之前的正则式指定匹配 m-n个重复 ,在 m 和 n 之间取尽量多 {m-n}? 上一个修饰符的非贪婪模式，只匹配尽量少的字符次数 \\ 转义特殊字符 [] 用于表示一个字符集合 | A|B A 和 B 可以是任意正则表达式, 一旦 A 匹配成功， B 就不再进行匹配 b,B s,S d,D w,W Z... 等特殊转义字符详解见 re正则表达式操作 模块 re.compile(pattern, flags=0) 将正则表达式的样式编译为一个 正则表达式对象 （正则对象），可以用于匹配，通过这个对象的方法 match(), search() 通过 re.compile()编译后的样式，和模块级的函数会被缓存， 所以少数的正则表达式使用无需考虑编译的问题. import re a = '''aaa\\nbbbb''' tmp = re.compile(r'(.*)\\n(.*)') tmp.match(a) ## ## 等价于 re.match(pattern=r'(.*)\\n(.*)', string=a, flags=0) flags: re.A(re.ASCII) 让 \\w, \\W, \\b, \\B, \\d, \\D, \\s 和 \\S 只匹配ASCII，而不是Unicode re.DEBUG 显示编译时的debug信息 re.I(re.IGNORECASE) 进行忽略大小写匹配 re.S(re.DOTALL) 让 '.' 特殊字符匹配任何字符，包括换行符；如果没有这个标记，'.' 就匹配除了换行符的其他任意字符 re.search(pattern, string, flags=0 ) 扫描整个字符串找到匹配样式的第一个位置, 并返回一个相应的匹配对象 如果没有匹配，就返回 None re.match(pattern, string, flags=0 ) 如果 string 开始的0或者多个字符匹配到了正则表达式样式, 就返回一个相应的匹配对象 如果没有匹配，就返回 None import re a = '''aaa\\naab\\naaa''' tmp = re.compile(r'.*b\\n') tmp.search(a) ## tmp.match(a) ## None re.fullmatch(pattern, string, flags=0 ) 如果整个 string 匹配到正则表达式样式, 就返回一个相应的匹配对象 如果没有匹配，就返回 None import re a = '''aaa\\naaa\\naaa''' tmp = re.compile(r'.*\\n.*\\n.*') tmp.fullmatch(a) ## re.split(pattern, string, flags=0 ) 用 pattern 分开 string , 如果在 pattern 中捕获到括号，那么所有的组里的文字也会包含在列表里. 如果 maxsplit 非零， 最多进行 maxsplit 次分隔， 剩下的字符全部返回到列表的最后一个元素 re.split(r'\\W+', 'Words, words, words.', 1) # ['Words', 'words, words.'] re.findall(pattern, string, flags=0 ) 对 string 返回一个不重复的 pattern 的匹配列表， string 从左到右进行扫描，匹配按找到的顺序返回. 如果样式里存在一到多个组，就返回一个组合列表；就是一个元组的列表（如果样式里有超过一个组合的话）. 空匹配也会包含在结果里. import re a = '''aaa\\naab\\naac\\n''' re.findall(r'.+\\n.*?',a) ## ['aaa\\n', 'aab\\n', 'aac\\n'] re.finditer(pattern, string, flags=0 ) pattern 在 string 里所有的非重复匹配，返回为一个迭代器 iterator 保存了 匹配对象 . string 从左到右扫描，匹配按顺序排列. 空匹配也包含在结果里. import re a = '''aaa\\naab\\naac\\n''' c = [] [c.append(i.group(0)) for i in re.finditer(r'.+[a|b]\\n.*?',a)] print(c) ## ['aaa\\n', 'aab\\n'] re.sub(pattern, repl, string, count=0, flags=0) 返回通过使用 repl 替换在 string 最左边非重叠出现的 pattern 而获得的字符串. 如果样式没有找到，则不加改变地返回 string. repl 可以是字符串或函数；如为字符串，则其中任何反斜杠转义序列都会被处理. 如果 repl 是一个函数，那它会对每个非重复的 pattern 的情况调用. 这个函数只能有一个 匹配对象 参数，并返回一个替换后的字符串. re.subn(pattern, repl, string, count=0, flags=0) 行为与 sub() 相同，但是返回一个元组 (字符串, 替换次数). 正则表达式对象 编译后的正则表达式对象支持的方法和属性 pattern = re.compile(\"\\n\") Pattern.search(string[, pos[, endpos]]) Pattern.match(string[, pos[, endpos]]) Pattern.fullmatch(string[, pos[, endpos]]) Pattern.split(string[, pos[, endpos]]) Pattern.findall(string[, pos[, endpos]]) Pattern.finditer(string[, pos[, endpos]]) Pattern.sub(string[, pos[, endpos]]) Pattern.subn(string[, pos[, endpos]]) Pattern.flags 正则匹配标记 Pattern.groups 捕获组合的数量 Pattern.groupindex 映射由 (?P) 定义的命名符号组合和数字组合的字典 如果没有符号组, 那字典就是空的 Pattern.pattern 编译对象的原始样式字符串 匹配对象 匹配对象总是有一个布尔值 True. 如果没有匹配的话 match() 和 search() 返回 None 所以可以用 if 语句来判断是否匹配 import re a = '''aaa\\naab\\naac\\n''' x = re.match(r'.+\\n.*?',a) if x: print(x.group()) ## aaa Match.group([group1, ...]) 返回一个或者多个匹配的子组. 如果只有一个参数，结果就是一个字符串，如果有多个参数，结果就是一个元组（每个参数对应一个项） 如果没有参数，组1默认到0（整个匹配都被返回） import re a = '''aaa\\naab\\naaa''' tmp = re.compile(r'(.*\\n)(.*\\n).*') x = tmp.search(a) x.group() ## 'aaa\\naab\\naaa' x.group(0) ## 'aaa\\n' x.group(1) ## 'aab\\n' Match.groups(default=None) 返回一个元组，包含所有匹配的子组，在样式中出现的从1到任意多的组合. default 参数用于不参与匹配的情况，默认为 None. import re a = '''aaa\\naab\\naaa''' tmp = re.compile(r'(.*\\n)(.*\\n).*') x = tmp.search(a) x.groups() ## ('aaa\\n', 'aab\\n') Match.start([group]) Match.end([group]) 返回 group 匹配到的字串的开始和结束标号 x.start() ## 0 x.end () ## 11 Match.span([group]) 对于一个匹配 m, 返回一个二元组 (m.start(group), m.end(group)). 如果 group 没有在这个匹配中，就返回 (-1, -1). group 默认为0，就是整个匹配. x.span() ## (0, 11) x.span(1) ## (0, 4) x.span(2) ## (4, 8) Match.pos Match.endpos 正则引擎开始/停止在字符串搜索一个匹配的索引位置 Match.re 返回产生这个实例的 正则对象, 这个实例是由 正则对象的 match() 或 search() 方法产生的 Match.string 传递到 match() 或 search() 的字符串 "},"PythonShare/Numbers&Datatime.html":{"url":"PythonShare/Numbers&Datatime.html","title":"Numbers&Datatime","keywords":"","body":"Numbers&Datatime Numbers 数字的四舍五入 # 对于简单的舍入运算，使用内置的 round(value, ndigits) 函数 round(1.23, 1) round(1.27, 1) round(1.5,0) round(2.5,0) a = 1627731 round(a, -1) round(a, -2) # 简单的输出一定宽度的数,格式化的时候指定精度 x = 1.23456 format(x, '0.3f') print('value is {:0.3f}'.format(x)) 执行精确的浮点数运算 a = 4.2 b = 2.1 print(a + b==6.3) #底层 CPU 和 IEEE 754 标准通过自己的浮点单位去执行算术时导致 #decimal 模块更精确但损耗性能 from decimal import Decimal a = Decimal('4.2') b = Decimal('2.1') print(type(a + b)) Decimal('6.3')==a+b nums = [1.23e+18, 1, -1.23e+18] sum(nums) import math math.fsum(nums) 数字的格式化输出 x = 1234.56789 format(x, '0.2f') format(x, '>10.1f') format(x, ' 二八十六进制整数 # 为了将整数转换为二进制、八进制或十六进制的文本串，可以分别使用 bin(),oct() 或 hex() 函数： x = 1234 bin(x) oct(x) hex(x) # 不想输出 0b , 0o 或者 0x 的前缀的话，可以使用 format() 函数 format(x, 'b') format(x, 'o') format(x, 'x') # 不同的进制转换整数字符串，简单的使用带有进制的 int() 函数 int('4d2', 16) int('10011010010', 2) 字节到大整数的打包与解包 data = b'\\x00\\x124V\\x00x\\x90\\xab\\x00\\xcd\\xef\\x01\\x00#\\x004' ##字节字符串 #int.from_bytes(bytes, byteorder, *, signed=False) #参数解释： bytes是要转换的十六进制； #byteorder：选'big'和'little'，以上例为例，其中big代表正常顺序，即f1ff。little反之，代表反序fff1； #signed：选True、Flase表示是否要区分二进制的正负数含义。即是否要对原二进制数进行原码反码 补码操作。 int.from_bytes(data, 'little') int.from_bytes(data, 'big') x = 94522842520747284487117727783387188 x.to_bytes(16, 'big') x.to_bytes(16, 'little') x = 0x01020304 x.to_bytes(4, 'big') x.to_bytes(4, 'little') 复数的数学运算 j*2=-1 a = complex(2, 4) b = 3 - 5j a.real a.imag a.conjugate() ###共轭复数 两个实部相等，虚部互为相反数的复数互为共轭复数 a + b a * b a / b abs(a) import cmath cmath.sin(a) cmath.cos(a) cmath.exp(a) #Python 的标准数学函数确实情况下并不能产生复数值 math.sqrt(-1) cmath.sqrt(-1) 无穷大与 NaN #创建 a = float('inf') b = float('-inf') c = float('nan') d = float('nan') math.isinf(a) math.isnan(c) a + 45 a * 10 10 / a a/a a + b c + 23 c / 2 math.sqrt(c) print(c==d) 3.8,3.9,3.10 分数运算，大型数组运算，矩阵和线性代数运算 大家自己看看 随机选择 import random values = [1, 2, 3, 4, 5, 6] random.choice(values) ###一个元素 random.choice(values) random.sample(values, 2) ##多个元素 random.shuffle(values) ##打乱顺序 #生成随机整数 random.randint(0,10) #生成 0 到 1 范围内均匀分布的浮点数 random.random() #获取 N 位随机位 (二进制) 的整数 random.getrandbits(10) ###随机种子 random.seed(12345) Datatime 基本的日期与时间转换 from datetime import timedelta a = timedelta(days=2, hours=6) b = timedelta(hours=4.5) c = a + b c.days c.seconds c.seconds / 3600 c.total_seconds() / 3600 from datetime import datetime a = datetime(2012, 9, 23) print(a + timedelta(days=10)) #days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0 b = datetime(2012, 12, 21) d = b - a d d.days now = datetime.today() print(now) print(now + timedelta(minutes=10)) from dateutil.relativedelta import relativedelta a + relativedelta(months=+1) d = relativedelta(b, a) 计算最后一个周五的日期 from datetime import datetime from dateutil.relativedelta import relativedelta from dateutil.rrule import * print(d + relativedelta(weekday=FR(3))) "},"PythonShare/IO&File.html":{"url":"PythonShare/IO&File.html","title":"IO&File","keywords":"","body":"IO&File IO官方文档 异常类官方文档 IO在计算机中指Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由CPU这个超快的计算核心来执行，涉及到数据交换的地方，通常是磁盘、网络等，就需要IO接口 stream(流) 所有流对于提供给他们的数据的数据类型都有严格要求, 如果使用二进制流的write()方法写入字符类型的数据,就会引发 TypeError 同步IO与异步IO 因为CPU写入磁盘的速度远远超过 磁盘接收的速度 如果CPU暂停程序等待写入磁盘完成后再执行后续代码,则为同步IO 如果CPU只是告诉磁盘让ta慢慢写, 自己去干别的事了, 则为异步IO 缓存 写入磁盘需要物理开销, 且速度较慢, 对于磁盘IO来说, 策略为留出一部分空间用做磁盘缓存,等磁盘缓存写满了再将其写入磁盘 Python IO模块的IO类型 创造流最简单的方式就是使用 open()函数, 指定对应的编码格式即可创建对应类型的流 Text I/O (文本IO) path = 'e:/Desktop/alongtest.sql' f = open(path, 'r', encoding= 'utf-8') f = io.StringIO('hello world') Binary I/O (二进制IO) path = 'e:/Desktop/怎么回事小老弟.jpg' f = open(path, 'rb') f = io.BytesIO(b'hello world') Raw I/O (原始IO) # 不建议使用 path = 'e:/Desktop/怎么回事小老弟.jpg' f = open(path, 'rb', buffering = 0) I/O操作的类层次结构 ABC(abstract base classes)抽象基类 Inherits Stub Methods Mixin Methods and Properties IOBase fileno,seek,truncate close,closed,flush,isatty,__enter__,__exit__,__iter__,__next__, readable,readline,readlines,seekable,tell,writable,writelines RawIOBase IOBase readinto,write Inherits IOBase Methods, read, readall BufferedIOBase IOBase detach,read,readline,write Inherits IOBase Methods, readinto, readintoline TextIOBase IOBase detach,read,readline,write Inherits IOBase Methods, encoding, errors, newlines 1. IO最顶层的类是抽象基类 IOBase, 其定义了流的基本接口, 但是对于读取流和写入流未作区分 实现该基类时, 如果给定的操作没有实现, 则会引发 UnsupportedOperation 错误 2. RawIOBase继承自IOBase, 用于处理的从流中读取或者向流中写入字节 FileIO是RawIOBase的子类, 为文件系统中的文件提供接口 3. BufferedIOBase处理原始字节流(RawIOBase)上的缓冲, 其子类有 BufferdWriter,BufferedReader,BufferedRWPair等带缓冲区的流 分别是可读的流，可写的流，既可读又可写的流 BufferedRandow提供了一个带缓冲区的接口给随机访问流 BufferedIOBase的另一个子BytesIO是内存中的字节流 4. TextIOBase是IOBase的另一个子类，处理文本形式的字节流，并且处理相应的对字符串的编码和解码操作 TextIOWrapper是从TextIOBase中继承而来，是为带缓冲区的原始流提供的带缓冲区的文本接口 StringIO是内存中的文本流 IO模块类图 IOBase RawIOBase 无缓存的字节流 FileIO 操作系统文件流 BufferedIOBase 缓存的字节流 BytesIO BufferedReader BufferedWriter BufferedRandom BufferedRWPair TextIOBase 编码相关的文本流 StringIO 文本的内存流 TextIOWrapper IO模块的文本IO之StringIO类 文本I/O被读取后，就是在内存中的流 这样的内存流，在调用close()方法后释放内存缓冲区 StringIO类参数 initial_value = '' 缓冲区初始值 newline = '\\n' 换行符 StringIO类额外的方法 getvalue() 返回一个str，包含整个缓冲区的内容 StringIO类的用法 from io import StringIO output = StringIO() output.write('First line.\\n')#写入第一行 print('Second line.', file=output)#写入第二行 print(output.getvalue()) output.close() 补充 1. pickle ## 几乎可以将所有的python对象转化成二进制的形式 import pickle import pandas as pd ## 保存 df = pd.DataFrame({'a':1,'b':2},index=['along']) with open('e:/Desktop/test.pkl','wb') as pickle_file: pickle.dump(df,pickle_file) ## 取数 with open('e:/Desktop/test.pkl','rb') as pickle_file: data = pickle.load(pickle_file) data 2. json # 如果是文件 对应函数应为 dump(), load() import json d = dict(name='Bob', age=20, score=88) json.dumps(d) --'{\"name\": \"Bob\", \"age\": 20, \"score\": 88}' from collections import OrderedDict s = '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}' data = json.loads(s, object_pairs_hook=OrderedDict) data 3. 操作文件和目录 import os os.name # 操作系统类型 os.uname() # 详细的系统信息 os.path.abspath('.') # 查看当前目录的绝对路径 os.path.join('/Users', 'testdir') #在某个目录下创建一个新目录，首先要把新目录的完整路径表示出来 os.mkdir('/Users/testdir') #创建一个目录 os.rmdir('/Users/testdir') #删除一个目录 os.path.split('/Users/file.txt') # 把一个路径拆分为两部分，后一部分总是最后级别的目录或文件名 os.path.splitext('/path/to/file.txt') # 拆分文件拓展名 os.rename('test.txt', 'test.py') # 对文件重命名 os.remove('test.py') # 删除文件 os.path.isdir('test') # 判断是否为目录,一般结合列表推导式使用 [name for name in os.listdir('e:/') if os.path.isdir(os.path.join('e:/', name))] # 筛选出文件夹 [name for name in os.listdir('e:/') if name.endswith('.py')]# 删选出所有python文件 4. open与with open f = open(r'E:/Desktop/alongtest.txt','r',encoding='utf-8') f.read() f.close() # 直接使用open方法需要调用close方法,建议使用以下方法 # 因为with 后面跟的语句返回一个对象, with要求改对象必须有一个__enter__()和__exit__()方法 with open(r'E:/Desktop/alongtest.txt','r',encoding='utf-8') as f: f.read() r read w write a additional b binary 读写都有 "},"PythonShare/Function.html":{"url":"PythonShare/Function.html","title":"Function","keywords":"","body":"Function 函数的参数 函数的参数共有以下几种： 位置参数； 默认参数； 可变参数； 关键字参数； 命名关键字参数； 位置参数 def power(x): return x * x 参数 x 即是一个位置参数 def power(x, n): s = 1 while n > 0: n = n - 1 s = s * x return s x和n，这两个参数都是位置参数，调用函数时，传入的两个值按照位置顺序依次赋给参数x和n； 默认参数 #默认参数的值仅仅在函数定义的时候赋值一次 def power(x, n=2): s = 1 while n > 0: n = n - 1 s = s * x return s n为默认参数 可变参数 def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum calc([1, 2, 3]) def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum calc(1, 2, 3) ###已经有一个list或者tuple，要调用一个可变参数 nums = [1, 2, 3] calc(*nums) 关键字参数 可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw) person('Michael', 30) output: name: Michael age: 30 other: {} person('Bob', 35, city='Beijing') output: name: Bob age: 35 other: {'city': 'Beijing'} 命名关键字参数 对于关键字参数，函数的调用者可以传入任意不受限制的关键字参数。至于到底传入了哪些，就需要在函数内部通过kw检查。如果要限制关键字参数的名字，就可以用命名关键字参数 def person(name, age, *, city, job): print(name, age, city, job) 参数组合 在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw) def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw) 函数注解 def add(x:int, y:int) -> int: return x + y 匿名函数 add = lambda x, y: x + y 等同于 def add(x, y): return x + y names = ['David Beazley', 'Brian Jones', 'Raymond Hettinger', 'Ned Batchelder'] sorted(names, key=lambda name: name.split()[-1].lower()) funcs = [lambda x: x+n for n in range(5)] for f in funcs: print(f(0)) #实际效果是运行是 n 的值为迭代的最后一个值,匿名函数是在函数调用是绑定值 funcs = [lambda x, n=n: x+n for n in range(5)] for f in funcs: print(f(0)) #通过使用函数默认值参数形式，lambda 函数在定义时就能绑定到值 偏函数 #减少可调用对象的参数个数 from functools import partial def partial(func, *args, **keywords): def newfunc(*fargs, **fkeywords): newkeywords = keywords.copy() newkeywords.update(fkeywords) return func(*args, *fargs, **newkeywords) newfunc.func = func newfunc.args = args newfunc.keywords = keywords return newfunc #返回一个新的partial对象，该对象在调用时的行为将类似采用位置参数args和关键字参数keywords对func的调用 def spam(a, b, c, d): print(a, b, c, d) s1 = partial(spam, 1) ##位置参数 s1(2, 3, 4) s2 = partial(spam, d=42) ##关键字参数 s2(1, 2, 3) s3 = partial(spam, 1, 2, d=42) s3(3) 回调函数 回调函数：就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。 通俗理解就是：把一个函数作为参数传给另一个函数，第一个函数称为回调函数 #回调函数一般运用在python的多线程中 举个栗子： def computer(a, b, func): return func(a, b) def max(a, b): return [a, b][a b] def sum(a, b): return str(int(a) + int(b)) if __name__ == \"__main__\": a = input(\"请输入整数a:\") b = input(\"请输入整数b:\") res = computer(a, b, max) print(\"Max of \" + a + \" and \" + b + \" is \" + res) res = computer(a, b, min) print(\"Min of \" + a + \" and \" + b + \" is \" + res) res = computer(a, b, sum) print(\"Sum of \" + a + \" and \" + b + \" is \" + res) 以上四个函数，谁是回调函数？ 偏函数，回调函数组合 from multiprocessing import Pool import time def mycallback(x,y): with open(y, 'a+') as f: #这里是回调函数向文件中写入值 f.writelines(str(x)) def sayHi(num): return num if __name__ == '__main__': e1 = time.time() pool = Pool() for i in range(10): pool.apply_async(sayHi, (i,), callback=partial(mycallback,y='yxy.txt')) #池的多线程 pool.close() pool.join() e2 = time.time() print(float(e2 - e1)) "},"GitTips/":{"url":"GitTips/","title":"GitTips","keywords":"","body":"GitTips "},"GitTips/GitCommon.html":{"url":"GitTips/GitCommon.html","title":"GitCommon","keywords":"","body":"Git 基本的使用 git push 之前先 git pull 1.查看状态 $ git status 2.从线上拉下来最新的与自己的进行合并 $ git pull $ git fetch --只拉取不合并 3.将自己的文件夹添加到暂存区 $ git add . 4.从暂存区将代码提交到 git本地仓库 $ git commit (--all) -m 'modify_reaon' 5.从本地仓库到远程仓库 $ git push [地址] [master(默认)] 6.设置用户名和邮箱 $ git config --global user.name(user.email) 'name/email' ## 1.取消文件修改 $ git checkout 2.查看提交日志 $ git log $ git log --oneline ##简洁版 4.查看所有版本提交日志 $ git reflog 3.回退版本(soft, mixed, hard, keep) $ git reset --hard $ git reset --hard 5.创建dev分支(刚创建时dev分支和master分支的东西是一样的) $ git branch dev 6.切换分支 $ git checkout dev 7.查看分支(输入命令打印后前面带*号的为当前分支) $ git branch 8.合并分支 $ git merge dev 9.删除分支(不能自己删除自己当前分支) $ git branch -d dev 10.生成公私钥(ssh) $ ssh-keygen -t rsa 11.将文件暂存起来放在暂存区(只能在add/commit之后才能暂存)(方便切换到其他分支工作) $ git stash 12.恢复暂存区文件 $ git stash apply (stash@{}n为第几个暂存区文件)(默认恢复最近的文件) $ git stash drop -- $ git stash pop (恢复最近的暂存区文件并删除缓存区中的该文件) $ git stash list (查看暂存区) 13.忽略提交(添加.gitignore文件) $ touch .gitignore $ vim .gitignore(编辑需要忽略的文件,支持正则) 14. 删除git全局配置项 $ git config --global --unset remote.origin.url 15. 设置git不需要输入密码 $ git config --global credential.helper store 16. 给代码添加tag $ git tag v1.11.0 -m\"v1.11.0\" $ git push origin v1.11.0 git命令 Git-tips "},"GitTips/QuicklyBuildGitBook.html":{"url":"GitTips/QuicklyBuildGitBook.html","title":"QuicklyBuildGitBook","keywords":"","body":"GitBook建立并结合GitHub使用 1 安装npm 2 全局安装GitBook npm install -g gitbook-cli 3 本地创建项目 例如PythonCookBook 4 执行命令 gitbook init && gitbook serve (默认4000端口 , 可指定) 5 查看本地启动的服务 127.0.0.1:4000 6 本地创建需要上传到远程仓库的.md文件并上传到远程仓库 git init git remote add origin https://github.com/github账号/项目名(需与本地目录一致).git git add . git commit -m 'init' git push -u origin master 7 本地进行测试并将_book推到供他人访问的分支上 1. 本地gitbook serve启动本地gitbook服务生成静态资源文件 访问 http://127.0.0.1:4000 2. git push 3. git subtree push --prefix=_book origin gh-pages 8 让他人访问 下面地址(对应的个人名字跟文件名改掉) https://yanyanglong.github.io/PythonShare/ 9 图片及主题设置 图片 1. 图片本地保存到某个位置, 例如along.png放到 /PythonShare/image 中 2. 引用时将图片路径导入,默认当前路径为根文件夹路径 故引用图片路径为 /image/along.png 写法为 ![PNG](/image/along.png), 同时在md文件的最开始加入以下内容 --- typora-root-url: .. --- 主题设置 1. 根目录下创建 book.json 文件,\"plugins\": []中写入插件名即可, \"theme-comscore\"为gitbook中一个主题包 详细个性化配置参考 http://www.chengweiyang.cn/gitbook/customize/book.json.html 2. css配置, 如代码行web页面显示长度(下例为个人配置) 根目录下创建website.css 文件,写入 .page-inner {max-width: 1000px;padding: 2px 2px 2px 2px;} 在book.json中 加入 \"styles\": { \"website\": \"website.css\" } - "},"GitTips/GitObject.html":{"url":"GitTips/GitObject.html","title":"GitObject","keywords":"","body":"Git对象 问题 git 在每次提交保存的是什么 .git目录是用来干什么的 git提交生成的像乱码一样的一串字符是什么 Git对象类型 块(blob--binary large object) 用来指代可以包含任意数据的变量或文件,其内部结构会被程序忽略. 一个blob被视为一个黑盒,一个blob保存一个文件的数据, 但不包含任何关于这个文件的元数据, 甚至没有文件名 目录树(tree) 一个目录树代表一层目录信息.记录了blob标识符,路径名和一个目录里所有文件的一些元数据.它可以递归引用其他目录树或子树对象来建立一个包含文件和子目录的完成层次结构 提交(commit) 一个提交对象保存版本库中每一次变化的元数据,包括作者,提交者,提交日期和日志信息. 每个提交对象指向一个目录数对象 标签(tag) 一个标签对象分配一个任意的且人类可读的名字给一个特定对象,通常是一个提交对象. 常用的标签例如 Version-1.0-Ha Git对象图解 Git二次提交对象 Git工作时对象的变化 blob git init find . ## 查看初始化git创建的内容 find ./objects/ echo 'hello world' > hello.txt git add hello.txt find ./objects/ ## 发现多了一个文件夹, 文件夹中的文件为一串字符 当创造了一个文件的时候,Git并不关心文件名,git只关心文件里的内容:'hello world'的字节和换行符. git对这个blob执行一些操作, 计算出它的SHA1散列值,并把散列值的十六进制表示作为文件名放入git对象库中 SHA1散列值保证了内容的唯一性,两个相等的SHA1散列值的概率为2^160(10^48)分之一,就算用一万亿人每秒产生一个blob对象的速度,也要一万亿年,才能产生10^43个blob对象 ## 你可以随时通过散列值从git对象库中取出其内容 git cat-file -p 3b18e512dba79e4c8300dd08aeb37f8e728b8dad(3b18 至少前4个字母) tree Git通过目录树(tree)的对象来跟踪文件的路径名.当使用git add 命令时, git 会立即为文件内容创建一个blob对象,但是不会为tree创建一个对象. git 会更新索引,索引用来跟踪文件的路径名和相应的blob,每次执行命令时,git会用新的路径名和blob信息来更新索引 ## 查看索引 git ls-files -s ## 捕获索引状态并更新tree对象 git write-tree ## 68aba62e560c0ebc3396e8ae9335232cd93a3f60 find .git/objects/ tree层次结构 ## 新建一个文件夹将hello.txt 放到新文件夹中 mkdir subdir cp hello.txt subdir/hello.txt git add subdir/hello.txt git write-tree ## 68aba62e560c0ebc3396e8ae9335232cd93a3f60 可以看到subdir的对象名仍然是上面生成的树对象, 如果你拥有的一个树对象与别人的树对象一模一样 你就可以判断, 你们两个的树对象是一模一样的 find .git/ojbects ## 这里有三个唯一对象, 一个包含 'hello world'的blob, ## 一个包含hello.txt的树,文件里是'hello world' ## 还有一颗树 包含了指向hello.txt的树对象和一个包含 'hello world'的blob ## 提交树对象 echo -n \"commit a file say hello\" | git commit-tree 492413 #158b8896e8ee7be2024cd13f698d4cf3d5520e29 git cat-file -p 158b8 ## 查看提交内容 git show --pretty=fuller 158b8 ## 查看详细提交内容 tag ## 为158b8 提交打标签V1.0 git tag -m 'Tag version 1.0' V1.0 158b8 git rev-parse V1.0 git cat-file -p e8b61a794bcf0188bc62d7bb89b5e20cb5b32343 "},"Linux/":{"url":"Linux/","title":"Linux","keywords":"","body":"Linux "},"Linux/SoftInstall/":{"url":"Linux/SoftInstall/","title":"SoftInstall","keywords":"","body":"SoftInstall "},"Linux/SoftInstall/python3.html":{"url":"Linux/SoftInstall/python3.html","title":"python3","keywords":"","body":"安装python3(3.7.5) # 安装编译相关工具 yum -y groupinstall \"Development tools\" yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel # 安装sasl报错时先安装以下依赖 yum -y install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64 # hive连接报错先安装 yum -y install cyrus-sasl-plain cyrus-sasl-devel cyrus-sasl-gssapi # 下载安装包并解压 cd /home/service wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tar.xz tar -xvJf Python-3.7.5.tar.xz # 编译安装 mkdir /usr/local/python3 #创建编译安装目录 cd /home/service/Python-3.7.5 ./configure --prefix=/usr/local/python3 make && make install # 创建软连接 ln -s /usr/local/python3/bin/python3 /usr/local/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/local/bin/pip3 # 验证是否成功 python3 -V pip3 -V # 指定pip源 vim ~/.pip/pip.conf [global] index-url=http://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com python3卸载 # 卸载python3 rpm -qa|grep python3|xargs rpm -ev --allmatches --nodeps # 卸载所有残余文件 whereis python3 |xargs rm -frv python3.x import ssl 报错 -- 报错原因为: centos使用的openssl最高级为1.1.0, 但是python3.x使用的openssl为1.1.x之上的版本 # 下载安装包并解压 openssl-1.1.1a cd /home/service wget https://www.openssl.org/source/openssl-1.1.1a.tar.gz tar -zxvf openssl-1.1.1a.tar.gz # 编译安装openssl-1.1.1a mkdir /usr/local/openssl-1.1.1a cd /home/service/openssl-1.1.1a ./config --prefix=/usr/local/openssl-1.1.1a make && make install # 配置openssl共享库, 编辑,更新,检验 vim /etc/ld.so.conf.d/openssl.conf /usr/local/openssl-1.1.1a/lib/ ldconfig ldconfig -v | grep ssl 出现 libssl.so.1.1 -> libssl.so.1.1 就表示成功了 # 重新编译python cd /home/service/Python-3.7.5 ./configure --prefix=/usr/local/python3 --with-openssl=/usr/local/openssl-1.1.1a make && make install ln -s /usr/local/python3/bin/python3 /usr/local/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/local/bin/pip3 #验证 python3 >>>import ssl 不报错就成功了 python3手动安装插件 wkhtmltopdf,wkhtmltoimage 官网下载 https://wkhtmltopdf.org/downloads.html # 查看当前服务器的发行版本 1. lsb_release -a 2. cat /etc/centos-release 3. cat /etc/issue # 下载包文件, 位置 /home/service wget https://github.com/wkhtmltopdf/wkhtmltopdf/releases/download/0.12.4/wkhtmltox-0.12.4_linux-generic-amd64.tar.xz cd /home/service tar -xvf wkhtmltox-0.12.4_linux-generic-amd64.tar.xz cp /home/service/wkhtmltox/bin/wkhtmltopdf /usr/local/bin cp /home/service/wkhtmltox/bin/wkhtmltoimage /usr/local/bin # 测试是否成功 wkhtmltopdf --version # wkhtmltopdf 0.12.4 (with patched qt) 即成功 "},"Linux/SoftInstall/MySQL.html":{"url":"Linux/SoftInstall/MySQL.html","title":"MySQL","keywords":"","body":"安装MySQL 此文针对centos发行版本的Linux系统 #安装mysql前, 先把系统自带的mariadb-lib卸载 rpm -qa|grep mariadb -- 例如结果为: mariadb-libs-5.5.44-2.el7.centos.x86_64 rpm -e --nodeps mariadb-libs-5.5.44-2.el7.centos.x86_64 #如果是新的机器,先安装一下wget yum install wgent -y 对mysql版本无要求 yum list mysql* yum install mysql -y yum install mysql-server -y service mysqld start #停止service mysqld stop service mysqld status #查看状态 cat /var/log/mysqld.log | grep password --获取安装时的用户密码,假设为-dz%-KLw0fe mysql -uroot -p -dz%-KLw0fe ALTER USER \"root\"@\"localhost\" IDENTIFIED BY \"123456\"; #修改密码 #在修改密码时会报错 \"Your password does not satisfy the current policy requirements\" ALTER USER \"root\"@\"localhost\" IDENTIFIED BY \"Root_123456\"; 先修改为指定强度的密码,然后修改密码强度 set global validate_password.policy=0; set global validate_password.length=1; ALTER USER \"root\"@\"localhost\" IDENTIFIED BY \"123456\"; #修改为简单密码 #设置允许外部通过root用户访问 GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456'; #刷新权限 flush privileges; 指定mysql版本 #准备好mysql的版本, 例如 mysql-5.7.31, 放置位置为 /home/service/ cd /home/service/ wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum install mysql57-community-release-el7-10.noarch.rpm -y yum install mysql-community-server -y #启动mysql systemctl start mysqld.service #检查mysql状态 systemctl status mysqld.service #如果查看mysql状态报错就根据报错百度解决 #查看mysql初始密码 cat /var/log/mysqld.log | grep 'password' #例如 %k#tUa7M8dP! mysql -uroot -p %k#tUa7M8dP! mysql> ALTER USER root@localhost IDENTIFIED BY \"d6DLgGST\"; 报错: Your password does not satisfy the current policy requirements 退出mysql 终端 vim /etc/my.cnf, 在末尾添加 #修改mysql密码策略允许弱密码 validate_password_policy=0 validate_password=off #重启mysql systemctl restart mysqld.service 进入mysql重新修改密码,成功 #设置允许外部通过root用户访问 GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'd6DLgGST'; #刷新权限 flush privileges; only_full_group_by问题 该问题为在5.7下mysql的model默认为ONLY_FULL_GROUP_BY, sql中select后面的字段必须出现在group by后面, 或者被聚合函数包裹, 不然就认为是错误的 编辑mysql配置文件 vim /etc/my.cnf.d/mysql-server.cnf 修改sql_mode为: sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION 重启mysql服务 systemctl restart mysqld 查看mysql服务 systemctl status mysqld 开启慢sql日志 show VARIABLES like '%general_log%'; -- 是否开启输出所有日志 show VARIABLES like '%slow_query_log%'; -- 是否开启慢SQL日志 show VARIABLES like '%log_output%'; -- 查看日志输出方式（默认file，还支持table） show VARIABLES like '%long_query_time%'; -- 查看多少秒定义为慢SQL set global log_output='table'; -- 日志输出到table（默认file） set global general_log=on; -- 打开输出所有日志 set global slow_query_log=on; -- 打开慢SQL日志 set global long_query_time=2; -- 设置2秒以上为慢查询 ##查看数据库表大小 1.查看所有数据库容量大小 select table_schema as '数据库’, sum(table_rows) as '记录数’, sum(truncate(data_length/1024/1024, 2)) as '数据容量(MB)’, sum(truncate(index_length/1024/1024, 2)) as '索引容量(MB)’ from information_schema.tables group by table_schema order by sum(data_length) desc, sum(index_length) desc; 2.查看所有数据库各表容量大小 select table_schema as '数据库’, table_name as '表名’, table_rows as '记录数’, truncate(data_length/1024/1024, 2) as '数据容量(MB)’, truncate(index_length/1024/1024, 2) as '索引容量(MB)’ from information_schema.tables order by data_length desc, index_length desc; 3.查看指定数据库容量大小 select table_schema as '数据库’, sum(table_rows) as '记录数’, sum(truncate(data_length/1024/1024, 2)) as '数据容量(MB)’, sum(truncate(index_length/1024/1024, 2)) as '索引容量(MB)’ from information_schema.tables where table_schema='mysql’; 4.查看指定数据库各表容量大小 select table_schema as '数据库’, table_name as '表名’, table_rows as '记录数’, truncate(data_length/1024/1024, 2) as '数据容量(MB)’, truncate(index_length/1024/1024, 2) as '索引容量(MB)’ from information_schema.tables where table_schema='mysql’ order by data_length desc, index_length desc; "},"Linux/SoftInstall/Git.html":{"url":"Linux/SoftInstall/Git.html","title":"Git","keywords":"","body":"升级git # 低于2.x的git有漏洞, 并且在git clone 的时候地址前需要加上 username@ # centos 有些默认会安装1.7.x的git, 升级更好用 git --version #检查版本 git version 1.7.1 # 安装依赖软件 yum install -y curl-devel expat-devel gettext-devel openssl-devel zlib-devel asciidoc yum install -y gcc perl-ExtUtils-MakeMaker # 卸载自带git版本(1.7.1) yum remove -y git # 编译安装最新的git版本 cd /home/service/soft_tar wget https://www.kernel.org/pub/software/scm/git/git-2.15.1.tar.xz tar -vxf git-2.15.1.tar.xz -C ../ cd /home/service/git-2.15.1 make prefix=/usr/local/git all make prefix=/usr/local/git install vim /etc/profile.d/git_env.sh export PATH=$PATH:/usr/local/git/bin source /etc/profile.d/git_env.sh # 检查版本 git --version #git version 2.15.1 "},"Linux/SoftInstall/ELK.html":{"url":"Linux/SoftInstall/ELK.html","title":"ELK","keywords":"","body":"ELK elasticsearch 1 官网下载: https://www.elastic.co/downloads/elasticsearch 例:下载位置 /Users/ysx_along/Downloads 2 解压 tar -xvf /Users/ysx_along/Download/elasticsearch-7.6.2-darwin-x86_64.tar.gz -C /Users/ysx_along/elk 3 运行 -- 运行会有个报错, 因为bash3.0之后加入新符号 ' kibana 1 官网下载, 版本要与elasticsearch保持一致 https://www.elastic.co/start 例:下载位置 /Users/ysx_along/Downloads 2 解压 tar -zxvf /Users/ysx_along/Download/kibana-7.6.2-darwin-x86_64.tar.gz -C /Users/ysx_along/elk 3 运行 -- kibana运行时会去找本地运行的elasticsearch, localhost运行可以不指定elasticsearch的url地址 配置对应的es地址 vim /Users/ysx_along/elk/kibana-7.6.2-darwin-x86_64/config/kibana.yml elasticsearch.hosts: [\"http://host:9200\"] /Users/ysx_along/elk/kibana-7.6.2-darwin-x86_64/bin/kibana 4 本地打开浏览器 http://localhost:5601 logstash 1 官网下载: https://artifacts.elastic.co/downloads/logstash/ 例:下载位置 /Users/ysx_along/Downloads 2 解压 tar -zxvf /Users/ysx_along/Download/logstash-7.8.0.tar.gz -C /Users/ysx_along/logstash 3 创建logstash.conf文件填入输入/处理/输出配置 vim /Users/ysx_along/elk/logstash-7.8.0/bin/logstash.conf input {} --输入源 filter {} --过滤条件 output {} --输出源 4 运行 #控制台测试 /Users/ysx_along/elk/logstash-7.8.0/bin/logstash -e 'input { stdin { } } output { stdout {} }' -- 控制台输入hello, 看到控制台输出 { \"@version\" => \"1\", \"host\" => \"172-10-30-128.lightspeed.rlghnc.sbcglobal.net\", \"message\" => \"hello \", \"@timestamp\" => 2020-07-14T05:48:56.250Z } #conf文件配置启动 /Users/ysx_along/elk/logstash-7.8.0/bin/logstash -f /Users/ysx_along/elk/logstash-7.8.0/bin/logstash.conf input { ##从控制台输入 stdin{} ##从日志文件中读取 # file { # path => [\"/Users/ysx_along/Desktop/analysis_project/DataMachine_for_web/ana_web_logs/ana_web_log_20200720.log\"] # type => \"along_test\" # tags => [\"ana_web日志\"] # start_position => \"beginning\" # } ## 如果有其他数据源, 直接在下面追加 #从kafka消费 # kafka { # bootstrap_servers => \"host:9092\" # topics => [\"along_test_topic\"] # codec => json { # charset => \"UTF-8\" # } # tags => [\"along_test_topic\"] # } ##从本地端口号5000的服务中读取 # tcp { # port => 5000 # # codec => json # tags => [\"ana_back_end\"] # } ##filebate端口接收 # beats { # port => 5044 # codec => \"json\" # } } ## 条件过滤 # filter { # # mutate { # # remove_tag => [\"_jsonparsefailure\"] # # remove_tag => [\"_grokparsefailure\"] # # } # grok { # match => { # \"message\" => \"%{DATA}\" # } # # \"message\" => \"%{message}\" # # \"version\" => \"%@version\" # # \"timestamp\" => \"%@timestamp\" # # \"tags\" => \"ana_kafka\" # } # } # } filter { # messge => \"%{message}\" # 将message转为json格式 json { source => \"message\" # target => \"message\" } } output { # 处理后的日志落到本地文件 # file { # path => \"/tmp/logstash_test.log\" # flush_interval => 0 # } # # 处理后的日志入es # elasticsearch { # hosts => \"host:9200\" # index => \"along_test_logstash\" # } # # 处理后的日志入hive # webhdfs { # host => \"host\" # port => 50070 # path => \"/user/hive/warehouse/ods.db/along_test_logstash/dt=%{+YYYYMMdd}\" # user => \"admin\" # } # # 处理后的日志入kafka # kafka { # bootstrap_servers => \"host:9092\" # topic_id => \"along_test_topic\" # codec => \"json\" # } # # 处理后的日志落到mysql 需要安装插件 /Users/ysx_along/elk/logstash-7.8.0/bin/logstash-plugin install logstash-output-jdbc # jdbc { # driver_jar_path => \"/Users/ysx_along/Downloads/mysql-connector-java-5.1.48/mysql-connector-java-5.1.48-bin.jar\" # driver_class => \"com.mysql.jdbc.Driver\" # connection_string => \"jdbc:mysql://host:3306/database?user=user&password=xxxxxxxxxxxxxx\" # statement => [\"insert into along_test(request_time,ip,host,host_url,method,path,full_path,request_body,request_args,headers,referrer) values (?,?,?,?,?,?,?,?,?,?,?)\",\"request_time\",\"ip\",\"host\",\"host_url\",\"method\",\"path\",\"full_path\",\"request_body\",\"request_args\",\"headers\",\"referrer\"] # } # 处理后的日志落到analyticdb 安装插件使插入速度提升 /Users/ysx_along/elk/logstash-7.8.0/bin/logstash-plugin install logstash-output-analyticdb # analyticdb { # driver_jar_path => \"/Users/ysx_along/Downloads/mysql-connector-java-5.1.48/mysql-connector-java-5.1.48-bin.jar\" # driver_class => \"com.mysql.jdbc.Driver\" # connection_string => \"jdbc:host:3306/dmart?user=user&password=password\" # statement => [\"insert into along_test(request_time,ip,host,host_url,method,path,full_path,request_body,request_args,headers,referrer) values (?,?,?,?,?,?,?,?,?,?,?)\",\"request_time\",\"ip\",\"host\",\"host_url\",\"method\",\"path\",\"full_path\",\"request_body\",\"request_args\",\"headers\",\"referrer\"] # commit_size => 4194304 # } # 打印样式 stdout { codec => rubydebug } } filebeat 1 官网下载, https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.8.0-darwin-x86_64.tar.gz 例:下载位置 /Users/ysx_along/Downloads 2 解压 tar -zxvf /Users/ysx_along/Download/filebeat-7.8.0-darwin-x86_64.tar.gz -C /Users/ysx_along/elk 3 配置yml文件 例:nginx.yml 4 运行 /Users/ysx_along/elk/filebeat-7.8.0-darwin-x86_64/filebeat -e -c /Users/ysx_along/elk/filebeat-7.8.0-darwin-x86_64/nginx.yml 5 如将filebeat推到logstash中,需在logstash.conf中配置 input { ##filebate端口接收 beats { port => 5044 # codec => \"json\" } } output { # 打印样式 stdout { codec => rubydebug } } 启动logstash /Users/ysx_along/elk/logstash-7.8.0/bin/logstash -f /Users/ysx_along/elk/logstash-7.8.0/bin/logstash.conf #nginx.yml #=========================== Filebeat inputs ============================= filebeat.inputs: - input_type: log paths: - /Users/ysx_along/data/logs/nginx_logs/ana_web.*.log #exclude_lines: [\"^DBG\"] #以什么开头的不收集 #include_lines: [\"^ERR\", \"^WARN\"] #只收集以什么开头的 #exclude_files: [\".gz$\"] #.gz结尾不收集 #document_type: \"nginx_filebeat\" #增加一个type #================================ Outputs ===================================== # Configure what outputs to use when sending the data collected by the beat. # Multiple outputs may be used. ##日志控制台打印 output.console: enabled: true pretty: true ##日志收集写入到logstash #output.logstash: # hosts: [\"host:5044\"] #logstash 服务器地址可写入多个 # enabled: true #是否开启输出到logstash 默认开启 # worker: 1 #进程数 # compression_level: 3 #压缩级别 # #loadbalance: true #多个输出的时候开启负载 #output.kafka: # enabled: true # codec.format: # string: '%{[message]}' # hosts: [\"host:9092\"] # topic: \"along_test_topic\" kafka 1 官网下载: https://mirrors.bfsu.edu.cn/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz 例:下载位置 /Users/ysx_along/Downloads 2 解压 tar -zxvf /Users/ysx_along/Download/kafka_2.12-2.5.0.tgz -C /Users/ysx_along/kafka 3 运行kafka需要启动Zookeeper, 如果没有Zookeeper可以kafka自带打包和配置好的Zookeeper /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/zookeeper-server-start.sh /Users/ysx_along/kafka/kafka_2.12-2.5.0/config/zookeeper.properties 4 启动kafka /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-server-start.sh /Users/ysx_along/kafka/kafka_2.12-2.5.0/config/server.properties #创建topic /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic along_test_topic #查看已创建的topic /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-topics.sh --list --zookeeper localhost:2181 #查看某个topic的具体分区,副本,leader信息 /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic along_test_topic #生产消息 /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic along_test_topic #消费消息 /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic along_test_topic --from-beginning #设置多个broker集群 cp /Users/ysx_along/kafka/kafka_2.12-2.5.0/config/server.properties config/server-1.properties cp /Users/ysx_along/kafka/kafka_2.12-2.5.0/config/server.properties config/server-2.properties 修改以下信息: config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1 config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2 然后分别启动 /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-server-start.sh /Users/ysx_along/kafka/kafka_2.12-2.5.0/config/server1.properties /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-server-start.sh /Users/ysx_along/kafka/kafka_2.12-2.5.0/config/server2.properties #创建一个新的topic,备份数设置为3 /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic #查看集群在做什么 /Users/ysx_along/kafka/kafka_2.12-2.5.0/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic "},"Linux/SoftInstall/xxl-job.html":{"url":"Linux/SoftInstall/xxl-job.html","title":"xxl-job","keywords":"","body":"安装 xxl-job 官方中文安装教程: https://github.com/xuxueli/xxl-job/blob/master/doc/XXL-JOB%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3.md # 安装mysql并设置账号密码如 root 123456 # clone 项目到本地, 位置 /home/service git clone https://github.com/xuxueli/xxl-job.git # 登陆mysql创建xxl-job需要的表 mysql -uroot -p123456 >source /home/service/xxl-job/doc/db/tables_xxl_job.sql # 配置调度中心(着重介绍需要注意的) vim /home/service/xxl-job/xxl-job-admin/src/main/resources/application.properties 1. server.port=8080 需要检查当前服务器的8080端口是否被占用, 如果被占用, 需要停止8080端口运行的服务/更改xxl-job调度中心的端口 # 检查当前可用80端口 netstat -ntulp | grep 80 # 查看端口对应进程 (假设pid为 20224) ps aux | grep 20224 2. spring.datasource.url=jdbc:mysql://127.0.0.1:3306/xxl_job?Unicode=true&characterEncoding=UTF-8 jdbc链接, 链接地址与 登陆mysql创建xxl-job表的地址一致, 建议填写ip地址 3. xxl.job.logretentiondays=30 调度中心日志表数据保存天数, 过期自动清理, -1关闭自动清理. 默认30天清理建议改为-1 # 配置执行器(着重介绍需要注意的) vim /home/service/xxl-job/xxl-job-executor-samples/xxl-job-executor-sample-springboot/src/main/resources/application.properties 1. server.port=8081 同调度中心检查端口,如有冲突则进行解决 2. xxl.job.executor.logretentiondays=30 执行器日志文件数据保存天数, 过期自动清理, -1关闭自动清理. 默认30天清理建议改为-1 # 使用maven将项目编译打包 cd /home/service/xxl-job mvn -U clean package -Dmaven.test.skip=true -- 打包完成后会生成target文件生成可执行的jar文件 # 部署 调度中心启动并在后台执行: nohup java -jar /home/service/xxl-job/xxl-job-admin/target/xxl-job-admin-2.2.1-SNAPSHOT.jar & 执行器启动并在后台执行: nohup java -jar /home/service/xxl-job/xxl-job-executor-samples/xxl-job-executor-sample-springboot/target/xxl-job-executor-sample-springboot-2.2.1-SNAPSHOT.jar & # 没有报错及部署成功 # 完成部署, 到 http://127.0.0.1:8080/xxl-job-admin 进行查看使用 ## 一台服务器上配置多个执行器时 启动完一个执行器后, 更改 /home/service/xxl-job/xxl-job-executor-samples/xxl-job-executor-sample-springboot/src/main/resources/application.properties 中的 port, xxl.job.executor.port, xxl.job.executor.appname 然后启动执行器 nohup java -jar -Dserver.port=8082 -Dxxl.job.executor.port=9998 /home/service/xxl-job/xxl-job-executor-samples/xxl-job-executor-sample-springboot/target/xxl-job-executor-sample-springboot-2.2.1-SNAPSHOT.jar & 安装java(jdk1.8) 下载地址: https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html wget https://repo.huaweicloud.com/java/jdk/8u201-b09/jdk-8u241-linux-x64.tar.gz 下载位置: /home/service/jdk-8u241-linux-x64.tar.gz tar -zxvf jdk-8u241-linux-x64.tar.gz 配置环境变量: vim /etc/profile.d/java_env.sh #!/bin/bash export JAVA_HOME=/home/service/jdk1.8.0_241 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin source /etc/profile.d/java_env.sh 检查java版本 java -version #java version \"1.8.0_241\" 安装maven 下载地址: https://maven.apache.org/download.cgi wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz 安装maven需要先安装jdk, maven3.3+需要jdk1.7及上, 具体见下载页 System Requirements # 下载位置: /home/service/apache-maven-3.6.3-bin.tar.gz cd /home/service tar -zxvf apache-maven-3.6.3-bin.tar.gz vim /etc/profile.d/maven_env.sh #!/bin/bash export MVN_HOME=/home/service/apache-maven-3.6.3 export PATH=$PATH:$MVN_HOME/bin source /etc/profile.d/maven_env.sh # 查看是否配置成功 mvn help:system # [INFO] BUILD SUCCESS 打印以上即成功 # maven打包命令 mvn -U clean package -Dmaven.test.skip=true "},"Linux/SoftInstall/airflow.html":{"url":"Linux/SoftInstall/airflow.html","title":"airflow","keywords":"","body":"安装Airflow 官方文档参考 1.直接pip3安装 pip3 install apache-airflow 安装好的airflow会在对应的python, 例如/usr/local/python3/bin/airflow 提前添加python环境变量 vim /etc/profile.d/python3_env export PATH=${PATH}:/usr/local/python3/bin 2. 修改配置 安装好的airflow, 默认文件目录为当前用户的根目录下 cd ~/airflow cp airflow.cfg airflow.cfg.bak vim ~/airflow/airflow.cfg 一些需要修改的配置项: (1). 日志位置 # The folder where airflow should store its log files # This path must be absolute base_log_folder = /data/logs/airflow/logs # How often should stats be printed to the logs print_stats_interval = 30 child_process_log_directory = /data/logs/airflow/logs/scheduler (2). 时区设置 default_timezone = Asia/Shanghai default_ui_timezone = Asia/Shanghai 在airflow包里修改 cd /usr/local/python3/lib/python3.7/site-packages/airflow/utils vim airflow/utils/timezone.py 在 utc = pendulum.timezone('UTC') 这行(第27行)代码下添加 from airflow import configuration as conf # from airflow.configuration import conf 最新版的修改方式 try: tz = conf.get(\"core\", \"default_timezone\") if tz == \"system\": utc = pendulum.local_timezone() else: utc = pendulum.timezone(tz) except Exception: pass 修改utcnow()函数 (在第69行) 原代码 result = dt.datetime.utcnow() 修改为 result = dt.datetime.now() vim airflow/utils/sqlalchemy.py 在utc = pendulum.timezone(‘UTC’) 这行(第37行)代码下添加 from airflow import configuration as conf # from airflow.configuration import conf 最新版的修改方式 try: tz = conf.get(\"core\", \"default_timezone\") if tz == \"system\": utc = pendulum.local_timezone() else: utc = pendulum.timezone(tz) except Exception: pass (3). ip和端口号 # The ip specified when starting the web server web_server_host = 0.0.0.0 # The port on which to run the web server web_server_port = 8080 (4). 数据库连接 sql_alchemy_conn = mysql+pymysql://airflow:password@数据库地址:3306/airflow (需要安装pymysql包, pip3 install pymysql) 创建airflow账号和密码和airflow数据库 (airflow_123) create user 'airflow'@'%' identified by 'password'; grant all privileges on airflow.* to 'airflow'@'%'; flush privileges; set global explicit_defaults_for_timestamp = 1; create database airflow CHARACTER SET = utf8mb4; (5). 加载dags文件的目录 dags_folder = /root/airflow/dags (如果通过项目管理, 可以设置为项目对应的文件夹目录或者使用软连接 ln -s /home/data-machine-for-wh/src/airflow_dags /root/airflow/dags) (6). 设置调度执行器类型 executor = LocalExecutor (注: CeleryExecutor 需要安装所需要的python包 redis, flower [pip3 install 'apache-airflow[celery]'], 且该版本与3.7的包冲突, 需要降低python版本为3.6方可使用; 启动方式为 airflow celery worker; airflow celery flower) 详情参照 http://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html 3. 初始化数据库 airflow db init 4. 创建admin账号 airflow users create --username admin --firstname admin --lastname admin --role Admin --email admin@keenon.com 输入密码 admin 5. 启动airflow airflow webserver (-D 以守护进程形式启动) airflow scheduler (-D 以守护进程形式启动) "},"Linux/SoftInstall/redis.html":{"url":"Linux/SoftInstall/redis.html","title":"redis","keywords":"","body":"Redis安装 1. 进入目录 cd /home/service/soft_tar 2. 下载安装包 wget http://download.redis.io/releases/redis-6.0.8.tar.gz 3. 解压 tar -zxvf redis-6.0.8.tar.gz -C ../ 4. 进入目录编译 cd /home/service/redis-6.0.8 make PREFIX=/usr/local/redis install 5. 运行 cd ./src ./redis-server 自定义配置 vim /home/service/redis-6.0.8/redis.conf 以自定义配置启动 cd /home/service/redis-6.0.8/src ./redis-server ../redis.conf make报错解决: make[1]: *** [server.o] 错误 1 make[1]: 离开目录“/home/service/redis-6.0.8/src” make: *** [all] 错误 2 原因为gcc版本过老, 检查gcc版本 gcc -v --gcc 版本 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) 升级gcc yum -y install centos-release-scl yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils scl enable devtoolset-9 bash 检查gcc版本 gcc -v gcc version 9.3.1 20200408 (Red Hat 9.3.1-2) (GCC) 重新编译 make PREFIX=/usr/local/redis install "},"Linux/LinuxTips.html":{"url":"Linux/LinuxTips.html","title":"LinuxTips","keywords":"","body":"LinuxTips 查看当前服务器的配置 # 查看当前操作系统版本信息 cat /proc/version # 查看当前操作系统内核信息 uname -a # 查看当前操作系统发行信息 cat /etc/centos-release or cat /etc/issue # 查看cpu相关信息 cat /proc/cpuinfo # 查看cpu物理核数 cat /proc/cpuinfo| grep \"cpu cores\"| uniq # 查看cpu逻辑核数 cat /proc/cpuinfo| grep \"processor\"| wc -l # 查看内存使用情况 free -h # 查看磁盘空间分区情况 fdisk -l # 查看文件系统磁盘空间使用情况及挂载点 df -h kill进程 # 查看实时进程占用资源信息 top (q退出) # 查看python3应用程序进程 ps aux | grep python3 # 找到第二行进程pid kill # 批量杀死进程快捷操作 ps aux | grep python3 | grep -v grep | awk '{print \"kill -9 \" $2}' | sh 增加磁盘空间 # 当某些分区磁盘空间不足时,有时需要增加, 这时就要根据分区的挂载点动态增加磁盘空间 # 参考博客: https://blog.csdn.net/zcc1229936385/article/details/81737576 1 给服务器增加磁盘空间, 例如增加10G 2 lsblk 查看, 会发现sda有一部分未做分区, 需要将新增的磁盘空间进行分区添加到我们需要的分区里 3 fdisk /dev/sda 对sda进行空间开垦 m -> n -> p -> 3 -> 回车 -> 回车 -> w m 帮助命令 n 添加新的分区 partition type 设置默认 p partiton_number(分区号)设置 3 扇区起始位置和结束扇区位置 w 保存 4 fdisk -l 查看分区情况, 此时 /dev/sda3为Linux类型, 需要改成 Linux LVM 卷(便于加入linux lvm然后将磁盘空间加入根分区) fdisk /dev/sda t -> 3 -> L -> 8e -> w 重启 reboot 5 将文件格式化为ext3格式 mkfs.ext3 /dev/sda3 6 扩充分区 pvcreate /dev/sda3 7 查看物理卷名, 将 /dev/sda3的 VG Name改成需要添加的逻辑分区的VG Name,例如需要添加的/dev/sda2: centos pvdisplay vgextend centos /dev/sda3 8 扩容,分配磁盘空间到需要的分区内, 注意不能完全把10G分配出去 lvextend -L +9.9G /dev/mapper/centos-root 9 重新调整逻辑分区大小 xfs_growfs /dev/mapper/centos-root 9 df -h 查看 其他 使用scp在Linux之间传输文件和目录 #从本地传输文件到远程 scp local_file remote_username@remote_ip:remote_folder 或者 scp local_file remote_username@remote_ip:remote_file 或者 scp local_file remote_ip:remote_folder 或者 scp local_file remote_ip:remote_file 第1,2个指定了用户名, 命令执行后需要再输入密码, 第1个仅指定了远程的目录, 文件名字不变，第2个指定了文件名; 第3,4个没有指定用户名, 命令执行后需要输入用户名和密码, 第3个仅指定了远程的目录, 文件名字不变, 第4个指定了文件名 #如果传输目录 则为scp -r ...... #从远程传输文件到本地 只需要将 scp后面的两个参数进行对调, 如果远程服务器防火墙为scp命令设置了指定端口, 使用-P参数指定端口 关闭防火墙和永久关闭 firewall-cmd --state 查看防火墙 systemctl stop firewalld.service 关闭防火墙 systemctl disable firewalld.service 永久关闭防火墙 关闭selinux setenforce 0 （临时生效） 修改 /etc/selinux/config 下的 SELINUX=disabled （重启后永久生效） 配置免密登陆 #集群修改host映射(每台机器都需要) vim /etc/hosts 192.168.1.101 node1 192.168.1.102 node2 ...... cd ./ssh #执行生成密钥命令 ssh-keygen -t rsa (四个回车) 将生成的公钥拷贝到要免登陆的机器上, 例如当前node1, 要登陆到node2上 ssh-copy-id -i ~/.ssh/id_rsa.pub node2 #验证 ssh node2 查看端口号是否被占用 netstat -apn | grep 4040 "},"Linux/LinuxAliases.html":{"url":"Linux/LinuxAliases.html","title":"Alias","keywords":"","body":"Linux别名 设置别名 alias --查看当前所有别名 alias 别名=\"实际执行的命令\" --添加别名 alias vi=\"vim\" unalias vi --取消vi的别名 unalias -a --取消所有别名 使用alias 直接设置别名并不会永久生效, 在系统重启后alias会重置,需要写到启动的配置文件中才能永久生效 1 获取root权限 2 进入启动配置文件aliases.sh中 vim /etc/profile.d/aliases.sh 3 添加别名 alias 别名=\"实际执行的命令\" 例 alias pushgitbook=\"git subtree push --prefix=_book origin gh-pages\" 注: **别名不能有空格, 等于号两边不能有空格** 4 source /etc/profile.d/aliases.sh 5 pushgitbook 执行的就是实际需要执行的命令了 参考文档 https://blog.csdn.net/sinat_34104446/article/details/83046269 "},"Linux/LinuxChapterOne_SearchCommand.html":{"url":"Linux/LinuxChapterOne_SearchCommand.html","title":"ChapterOne","keywords":"","body":"Linux--ChapterOne 环境变量 echo $PATH 当执行某一命令时,系统搜索命令的路径 shell命令快捷键 Ctrl + a --光标到行首 Ctrl + e --光标到行尾 Ctrl + u --删除光标之前的内容 Ctrl + l --清屏 相当于clear命令 文件搜索命令 locate 文件名 在后台数据库中按文件名搜索, 搜索速度快 /var/lib/mlocate -- locate命令所搜索的后台数据库 数据库每天更新一次, 新建文件需要更新数据库 更新逻辑见 /etc/updatedb.conf 配置文件 PRUNE_BIND_MOUNTS = \"yes\" --开启搜索限制 PRUNEFS --搜索时,不搜索的文件系统 PRUNENAMES -- 搜索时,不搜索的文件类型 PRUNEPATHS -- 搜索时,不搜索的路径 updatedb --更新数据库 find 搜索范围 搜索条件 应避免find命令大范围搜索,会非常消耗系统资源 find是在系统搜索符合条件的文件名, 如果需要匹配,使用通配符匹配,通配符是完全匹配 find /opt/case/analysis/ -name along.log find /opt/case/analysis/ -iname along.log --不区分大小写 find /opt/case/analysis/ -name \"along*\" --使用通配符 find /opt/case/analysis/ -user root --按照所有者搜索 find /opt/case/analysis/ -nouser --查找没有所有者的文件 find /opt/case/analysis/ -mtime +10 -- 查找十天前修改的文件 -10 10天内修改的文件 10 10天当天修改的文件 +10 10天前修改的文件 atime 文件访问时间 ctime 改变文件属性 mtime 修改文件内容 find /opt/case/analysis/ -size 12k --按照文件大小搜索 文件单位 k, M, G 注意只有k是小写,其他都是大写 -12k 小于12KB的文件 12k 等于12KB的文件 -12k 大于12KB的文件 find /opt/case/analysis/ -inum 134318202 --查找i节点为134318202的文件 find /opt/case/analysis/ -size +15k -a -size -20k -- 查找 /opt/case/analysis/ 目录下 大于15KB小于20KB的文件 -exec/ok 命令 {}\\; 规定写法,对搜索结果执行操作,改命令必须是可以处理搜索结果的命令 find /opt/case/analysis/ -size +15k -a -size -20k -exec ls -lh {} \\; 命令搜索命令 whereis/which 命令名 依赖环境变量 搜索命令的命令 whereis 命令名 搜索命令所在路径及帮助文档所在位置 -b 只查找可执行文件 -m 只查找帮助文档 which 命令名 搜索命令所在位置及别名 搜索字符串命令 grep [选项] 字符串 文件名 -i 忽略大小写 -v 排除指定字符串 包含搜索,如果需要匹配,使用正则表达式进行匹配, 正则表达式时包含匹配 grep \"import\" /opt/case/analysis/DataMachine_for_clean/src/along_test.py --搜索包含import的行 grep -v \"import\" ./along_test.py --搜索不包含import的行 帮助命令 man(manuals) man ls NAME 命令名称及功能简要说明 SYNOPSIS 用法说明，可用的选项 DESCRIPTION命令功能的详细说明及意义 SEE ALSO 另外参照 man的级别 1：查看命令的帮助 2：查看可被内核调用的函数的帮助 3：查看函数和函数库的帮助 4：查看特殊文件的帮助（主要是/dev目录下的文件） 5：查看配置文件的帮助 6：查看游戏的帮助 7：查看其它杂项的帮助 8：查看系统管理员可用命令的帮助 9：查看和内核相关文件的帮助 man -f 命令 --查看命令级别和功能简要说明 相当于whatis 命令 man -f ls; man -f passwd man -k passwd --查找所有关键字包含passwd的帮助文档 相当于apropos passwd 命令 --help 如果安装Linux指定语言为中文,则显示中文帮助文档 help shell内部命令 info 会找到所有的帮助文档(各个版本) "},"Linux/LinuxChapterTwo_FileFolder.html":{"url":"Linux/LinuxChapterTwo_FileFolder.html","title":"ChapterTwo","keywords":"","body":"Linux--ChapterTwo 文件/文件夹 创建 文件 touch 文件名 vim/vi 文件名 wq echo \"字符串\" > 文件名 echo \"字符串\" >> 文件名 >将左边的命令结果写入到右边的文件, >>为追加 cp 文件--one 文件--two mv 文件--old 文件--new 文件夹 mkdir 文件夹名 mv 文件夹--old 文件夹--new 软链接文件 ln -s [-i ] 源文件 目标文件 删除 Linux没有回收站 一旦删除会无法找回 使用rm -rf一定要注意! ! ! 文件 rm 文件名 文件夹 rm -rf 文件夹名 #删除目录下和子目录下所有文件,保留目录结构 find /home/work/ -type f -exec rm {} \\; #删除目录和子目录下所有temp文件, 但是保留文件夹 find ./ -name \"*.temp\" | xargs rm 统计 #统计目录下所有文件数量(不包含子目录) ls -al | grep \"^-\" | wc -l #统计目录下所有文件数量(包含子目录 -R递归) ls -alR | grep \"^-\" | wc -l #统计目录下所有目录数量(不包含子目录) ls -al | grep \"^d\" | wc -l #统计目录下所有目录数量(包含子目录 -R递归) ls -alR | grep \"^d\" | wc -l #统计目录大小 du -sh #统计当前目录大小,并按文件大小排序 du -sh * | sort -n #查看指定文件大小 du -shk [filename] 压缩/解压缩 文件 zip 压缩文件名 源文件 --压缩为.zip格式压缩文件,保留源文件 gzip 源文件 --压缩为.gz格式压缩文件,不保留源文件 gzip -c 源文件 > 压缩文件 --保留源文件 bzip2 源文件 --压缩为.zip格式压缩文件,不保留源文件 bzip2 -k 源文件 --保留源文件 bzip2不能压缩目录 文件夹 zip -r 压缩文件名 源目录 gzip -r 源目录 --压缩目录下所有子文件,但是不能压缩目录 解压缩 zip unzip 压缩文件 gzip gzip -d 压缩文件 gunzip 压缩文件 bzip2 bzip2 -d 压缩文件 -- -k保留压缩文件 bunzip2 压缩文件 -- -k保留压缩文件 打包(解包)后压缩(解压) 打包 tar -cvf 打包文件名 源文件 -c 打包 -v 显示过程 -f 指定打包后的文件名 tar -cvf tar_test.tar along.log 解包 tar -xvf 打包文件名 -x 解打包 tar -xvf tar_test.tar 打包并压缩 tar -zcvf 后缀为.tar.gz的压缩文件 源文件 tar -jcvf 后缀为.tar.bz2 的压缩文件 源文件 -z 压缩为.tar.gz 格式 -j 压缩为.tar.bz2 格式 解压并解包 tar -zxvf 后缀为.tar.gz的压缩文件 tar -jxvf 后缀为.tar.bz2 的压缩文件 "},"Linux/LinuxChapterThree_AuthorityConfiguration.html":{"url":"Linux/LinuxChapterThree_AuthorityConfiguration.html","title":"ChapterThree","keywords":"","body":"Linux--ChapterThree Authority 查看权限 ls -l /dev/ | more #打印的第一列内容如下 -rw-r--r-- 文件类型及权限 - 文件类型(-文件 d目录 l软连接文件) #不常用几乎见不到(b块设备文件,c字符设备文件,s套接字文件,p管道文件) rw- r-- r-- u所有者 g所属组 o其他人 r读 w写 x执行 修改权限 chmod [选项] 模式 文件名 选项 -R 递归 模式 [ugoa][+-=][rwx] [mode=421] chmod [ugoa][+-=][rwx] 文件名 #给[所有者|所属组|其他人][赋予|收回][读|写|执行]权限 chmod [ugoa]=[rwx] 文件名 #同时更改所有者|所属组|其他人权限,中间用逗号隔开 权限数字表示 r ---- 4 w ---- 2 x ---- 1 rwxr-xr-x 755 chmod 755 文件名 常用权限数字 777最高权限 644所有者读写,所属组和其他人读和执行 755所有者读写执行,所属组和其他人读执行 读写执行权限对文件/目录的作用 权限对文件的作用: r:读取文件内容(cat more head tail) w:编辑新增修改文件内容(vi/vim echo) #对文件有写的权限不包含删除,因为其执行权限是上一级目录控制 x:可执行 权限对目录的作用: r:可以查询目录下文件名(ls) w:具有修改目录结构的权限,[新建|删除|重命名]此目录下的文件和目录,剪切(touch rm mv cp) x:可以进入目录(cd) 对文件来说,最高权限是x执行 对目录来说,最高权限是w写, 对目录来说,有意义的权限只有 0,5,7 原则:对于文件,少赋予x权限 对于目录,少赋予w权限 修改文件的所有者 chown 所有者 文件名 chown 所有者:所属组 文件名 chgrp 所属组 文件名 默认权限umask cat /etc/profile #umask在配置初始文件目录时控制权限 规则为最高权限 - umask值 #root用户默认为022,普通用户默认002 1 文件默认最高权限666, 目录默认最高权限777 2 换算规则为 (root用户为例) 文件 rw-rw-rw- - ----w--w- = rw-r--r-- 目录 rwxrwxrwx - ----w--w- = rwxr-xr-x Configuration 登陆时配置文件的加载顺序 登陆进入Linux系统时, 配置文件依次加载, 后写入的变量会造成覆盖 1. 每个文件加载完成后都会调用下一个文件进行执行 2. /etc/目录下的为全局配置文件 ~目录下的为当前用户本地配置文件 3. 正常登陆后依次加载全局配置文件和本地配置文件中的变量,若有重复后定义的将覆盖前定义的变量(绿色线路) 4. 若切换用户,如root切换为其他用户(蓝色线路) 5. 无论如何登陆都会加载语言文件(粉红色线路) "},"Linux/LinuxChapterFour_JobSwith.html":{"url":"Linux/LinuxChapterFour_JobSwith.html","title":"ChapterFour","keywords":"","body":"Linux--ChapterFour 程序后台与前台运行 将程序放到后台运行 1 ./task.sh & 在末尾加上`&`符号, 让程序在后台运行 2 Ctrl+z 先将程序暂停, bg %[number] 再将程序放在后台运行 ./task.sh (Ctrl+Z) [1]+ Stopped ./task.sh bg %1 [1]+ ./task.sh & 查看运行中的程序 1 jobs -l 查看所有运行的程序 2 ps aux | grep 关键字 使用关键字查看进程信息 将程序放到前台运行 fg %1 将程序终止 kill %1 批量kill进程 ps aux | grep python3 | grep along_test.py | grep -v grep | awk '{print \"kill -9 \"$2}' | sh 进程自动被kill的原因(oom-killer) 在内核检测到系统内存不足后, 会触发oom-killer, 挑选最占用内存的进程杀掉 # 查看最近被oom的进程 tail -f -n 100 /var/log/messages # Linux分配内存策略 Linux 内核根据应用程序的要求来分配内存, 进程实际上不会将分配的内存全部使用 为了提高性能,内核采用过度分配内存(over-commit-memory)的策略来简洁利用进程的空闲内存,提高内存使用效率 一般情况下没有问题,但是如果大多数进程都耗光了自己的内存,所有的内存之和大于物理内存, 就需要杀死一部分进程,一般来说会选内存占用最大的进程杀掉 # 挑选杀掉的进程 挑选的过程由linux/mm/oom_kill.c里的 oom_badness() 函数决定，挑选的算法很直接: 最占用内存的进程 # 避免被杀掉 1 oom_badness()函数会给每个进程打分, 根据points的高低来决定杀死哪个进程, 分数越低越不会被杀掉 2 point可以根据 adj调节, root权限的进程通常被认为很重要,不应该轻易杀掉,所以打分的时候可以得到 %3 的优惠 3 通过改变每个进程的oom_adj内核参数来使进程不容易被omm killer选中, 调整 oom_score_adj 参数(points 越小越不容易被杀) 找到进程pid ps aux | grep python3 ysx_along 2334 1.6 2.1 623800 4876 ? Ssl 09:52 0:00 /usr/bin/python3 查看进程对应的oom_score_adj 值 cat /proc/2334/oom_score_adj 0 echo -15 > /proc/2334/oom_score_adj "},"Linux/ssh免密登陆.html":{"url":"Linux/ssh免密登陆.html","title":"ssh免密登陆","keywords":"","body":"ssh配置免密登陆 假设两台机器, master(root@192.168.11.110) slave1(root@192.168.11.111) 在两台机器上分别配置host映射: vim /etc/hosts 192.168.11.110 master 192.168.11.111 slave1 [root@master ~]ssh-keygen #生成公钥和私钥 [root@master ~]scp /root/.ssh/id_rsa.pub root@192.168.11.111:/home #公钥传到slave1机器上 输入slave1机器上root用户的密码: [root@slave1 ~]cat /home/id_rsa.pub >> /root/.ssh/authorized_keys #slave1保存master公钥 [root@slave1 ~]ssh master # 验证免密登陆 同样的操作配置master免密登陆slave1 1. A机器上生成公钥和私钥 2. 将公钥从A传到B 3. B保存A的公钥信息到 /root/.ssh/authorized_keys 文件中 "},"Linux/Linux中文编码问题.html":{"url":"Linux/Linux中文编码问题.html","title":"Linux中文编码问题","keywords":"","body":"Linux中生成的图片中文编码问题 # 全局安装中文字体 yum -y groupinstall chinese-support yum -y groupinstall mkfontscale # 手动刷新缓存 mkfontscale mkfontdir fc-cache Matplotlib中文编码问题 在Linux上运行matplotlib时会因中文编码问题造成乱码 ,解决方案为下载一个字体包并将其导入到Linux上的matplotlib中 下载arial unicode ms.ttf 的字体 拷贝一份到matplotlib中 ,路径为 cp arial\\ unicode\\ ms.ttf /root/.virtualenvs/datatimer/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf 重命名 cd /root/.virtualenvs/datatimer/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf cp arial\\ unicode\\ ms.ttf Vera.ttf 删除字体缓存 cd ~/.cache/matplotlib/ rm -rf fontlist-v300.json python脚本中添加 import matplotlib.pyplot as plt plt.rcParams['font.family'] = ['Arial Unicode MS', 'sans-serif'] # 全局设置支持中文字体，默认 sans-serif "},"Linux/PySpark/":{"url":"Linux/PySpark/","title":"PySpark","keywords":"","body":"Pyspark "},"Linux/PySpark/pyspark_introduction.html":{"url":"Linux/PySpark/pyspark_introduction.html","title":"PySpark简介","keywords":"","body":"PySpark简介及环境搭建 简介 Apache Spark 是基于内存计算的批处理实时处理框架, 支持交互式查询和迭代式算法 Apache Spark 是Scala 语言实现的, 为了支持Python语言使用Spark, Apache Spark 社区开发了一个工具--PySpark, 利用PySpark中的Py4J库, 我们可以通过Python语言操作RDD 开发原因: python丰富的拓展库和行业使用python的人数众多 pyspark 提供 pyspark shell, 结合了python api 和spark core的工具, pyspark shell在启动时会初始化Spark环境 环境搭建 1 安装Java 2 下载并安装 Apache Spark 1. 下载链接(spark2.4.4-hadoop2.7): http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz 2. 解压并设置环境变量 tar -zxvf /Users/ysx_along/Downloads/spark-2.4.4-bin-hadoop2.7 mv spark-2.4.4-bin-hadoop2.7 /Users/ysx_along/spark/ vim ~/.bash_profile export SPARK_HOME=/Users/ysx_along/spark/spark-2.4.4-bin-hadoop2.7 export PATH=$PATH:/Users/ysx_along/spark/spark-2.4.4-bin-hadoop2.7/bin export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH export PATH=$PATH:/Users/ysx_along/bin:$SPARK_HOME/python source ~/.bash_profile 3. 启动 pyspark(/Users/ysx_along/spark/spark-2.4.4-bin-hadoop2.7/bin/pyspark) 出现 SparkSession available as 'spark'. 表示spark shell启动成功并初始化了spark环境 -- windows 安装pyspark 参考以下链接 https://www.cnblogs.com/twodoge/p/10740791.html 3 本地连接远程服务器上的hive数据 将远程服务器上的 mapred-site.xml hdfs-site.xml yarn-site.xml core-site.xml文件拉到本地的SPARK_HOME 的conf目录下 代码中指定 SPARK_HOME hive元数据地址 from pyspark.sql import SparkSession os.environ[\"SPARK_HOME\"] = \"/Users/ysx_along/spark/spark-2.4.4-bin-hadoop2.7\" spark = SparkSession.builder.appName('along_test').config('hive.metastore.uris', 'thrift://bj-bi-cdh-01:9083,thrift://bj-bi-cdh-02:9083,thrift://bj-bi-cdh-03:9083,thrift://bj-bi-cdh-04:9083').master('local').enableHiveSupport().getOrCreate() SparkContext 和 RDD SparkContext SparkContext 是所有spark功能的入口, 无论希望运行什么spark应用, 都需要初始化SparkContext来驱动程序执行. SparkContext类属性: \"\"\" master:Spark集群的入口url地址. appName:任务名称. sparkHome:Spark安装目录. pyFiles:.zip 或 .py 文件可发送给集群或添加至环境变量中. Environment:Spark Worker节点的环境变量. batchSize:批处理数量.设置为1表示禁用批处理, 设置0以根据对象大小自动选择批处理大小, 设置为-1以使用无限批处理大小. Serializer:RDD序列化器. Conf:SparkConf对象, 用于设置Spark集群的相关属性. Gateway:选择使用现有网关和JVM或初始化新JVM. JSC:JavaSparkContext实例. profiler_cls:可用于进行性能分析的自定义Profiler(默认为pyspark.profiler.BasicProfiler) \"\"\" class pyspark.SparkContext ( master = None, appName = None, sparkHome = None, pyFiles = None, environment = None, batchSize = 0, serializer = PickleSerializer(), conf = None, gateway = None, jsc = None, profiler_cls = ) 上述参数中, master和appName是最常用的参数, 几乎所有的应用都需要传入这两个参数 pyspark shell 统计字符个数 # 启动pyspark path_ = r\"/Users/ysx_along/spark/spark-2.4.4-bin-hadoop2.7\" logfile = f\"file://{path_}/README.md\" logdata = sc.textFile(logfile).cache() numAs = logdata.filter(lambda s: 'a' in s).count() numBs = logdata.filter(lambda s: 'b' in s).count() print(numAs, numBs) # 62 31 python 脚本 vim /Users/ysx_along/spark/spark_test.py from pyspark import SparkContext sc = SparkContext(\"local\", \"first app\") path_ = r\"/Users/ysx_along/spark/spark-2.4.4-bin-hadoop2.7\" logfile = f\"file://{path_}/README.md\" logdata = sc.textFile(logfile).cache() numAs = logdata.filter(lambda s: 'a' in s).count() numBs = logdata.filter(lambda s: 'b' in s).count() print(numAs,numBs) # 62 31 spark-submit /Users/ysx_along/spark/spark_test.py #在输出内容中找得到结果 62 31 #spark提交py脚本指定参数 # 指定python路径 --conf spark.pyspark.python=/usr/bin/python3 --conf spark.pyspark.driver.python=/usr/bin/python3 # 指定数据内存大小 --conf spark.rpc.message.maxSize=512 # 指定使用的资源 --driver-memory 2G --num-executors 4 --executor-memory 4G --executor-cores 4 /usr/bin/spark2-submit --conf spark.pyspark.python=/usr/bin/python3 --conf spark.pyspark.driver.python=/usr/bin/python3 --conf spark.rpc.message.maxSize=512 --driver-memory 2G --num-executors 4 --executor-memory 4G --executor-cores 4 /home/timers/DataMachine_for_wh/src/whTask/along_test_warehouse.py RDD RDD(Resilient Distributed Dataset)弹性式分布数据集, 是可以在多个节点上运行和操作的数据,从而实现高效并行计算. 一旦创建了RDD, 就无法对其进行修改 RDD的操作分为两种形式: Transformation(转换): 此类型的操作应用与一个RDD后会得到一个新的RDD 例如: map, filter, groupByKey, reduceByKey, sortByKey, join等 Action(动作): 此类型的操作应用于一个RDD后,用于指示Spark执行计算并将计算结果返回 例如: reduce, collect, count, first, take, saveAsTextFile, foreach, top, lookup RDD类定义: class pyspark.RDD ( jrdd, ctx, jrdd_deserializer = AutoBatchedSerializer(PickleSerializer()) ) RDD基础操作 alist = [\"scala\", \"java\", \"hadoop\", \"spark\", \"apache\", \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"] words = sc.parallelize (alist) -- count()函数返回RDD中元素的数量 print(words.count()) # 8 -- foreach函数接收一个函数作为参数，将RDD中所有的元素作为参数调用传入的函数 def function1(x): print(x) words.foreach(function1) -- filter函数传入一个过滤器函数，并将过滤器函数应用于原有RDD中的所有元素，并将满足过滤器条件的RDD元素存放至一个新的RDD对象中并返回 print(words.filter(lambda x: 'spark' in x).collect()) # ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark'] -- map函数传入一个函数作为参数，并将该函数应用于原有RDD中的所有元素，将所有元素针对该函数的输出存放至一个新的RDD对象中并返回 print(words.map(lambda x: (x, 1)).collect()) # [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)] -- reduce函数接收一些特殊的运算符，通过将原有RDD中的所有元素按照指定运算符进行计算，并返回计算结果, 例如累加 from operator import add print(sc.parallelize([1, 2, 3, 4, 5]).reduce(add)) # 15 -- join函数()对RDD对象中的Key进行匹配，将相同key中的元素合并在一起，并返回新的RDD对象 x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)]) y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)]) joined = x.join(y) print(joined.collect()) # [('hadoop', (4, 5)), ('spark', (1, 2))] -- cache()函数可以对RDD对象进行默认方式(memory)进行持久化 words.cache() print(words.persist().is_cached) # True "},"Linux/PySpark/rdd_introduction.html":{"url":"Linux/PySpark/rdd_introduction.html","title":"RDD介绍","keywords":"","body":"RDD Resilient Distributed DataSet, 弹性式分布数据集. spark中最基本的数据抽象 RDD三大特性 : 分区, 不可变, 并行操作 分区 每一个RDD包含的数据被存储在系统的不同节点上, RDD只是抽象意义的数据集合, 分区内部不存储实际的数据, 只存储它在改RDD中的index, 通过RDD的id和分区的index可以唯一确定对应的数据块, 然后通过底层接口提取对应的数据进行处理. 在集群中, 各个节点上的数据块尽可能的存储在内存中, 只有内存没有空间时才会放到硬盘存储, 这种存储方式最大化的减少了磁盘IO的开销 逻辑上可以将RDD理解成一个数组, 数组中的每个元素就代表一个分区(partition) 物理存储中,每个分区指向一个存储在内存或硬盘中的数据块(block) 不可变 每个RDD 仅可读, 其所包含的分区信息 不可变 只有对现有的RDD进行 Transformation(转化)操作, 才能得到新的RDD, 然后经过多次计算迭代得到我们想要的结果 不可变特性的好处: 节省内存空间,提高计算效率, 在RDD计算过程中, 不需要立即去存储计算出的数据, 只要记录每个RDD是经过哪些转化操作得到的, 只记录依赖关系即可 错误恢复更容易, 如果在计算中节点发生故障, 数据丢失, 可以直接依据依赖关系从上一步去重新计算RDD, 实现 弹性 并行操作 不同节点上的数据可以分别被处理, 天然支持并行处理 RDD的结构 Partitions Partitions代表RDD中数据的逻辑结果, 每个Partition会映射到对应节点内存或硬盘上的一个数据块 SparkContext SparkContext是所有Spark功能的入口, 代表与Spark节点的连接, 可以用来创建RDD对象以及在节点中的广播变量等等. 一个线程只有一个SparkContext SparkConf SparkConf是一些配置信息 Partitioner Partitioner 决定了 RDD 的分区方式, 目前两种主流的分区方式: Hash partioner 和 Range partitioner. Hash 就是对数据的 Key 进行散列分布, Rang 是按照 Key 的排序进行的分区. 也可以自定义 Partitioner Dependencies Dependencies 即依赖关系, 记录了该 RDD 的计算过程, 即这个 RDD 是通过哪个 RDD 经过怎么样的转化操作得到的, 根据RDD分区的计算方式, 分为窄依赖和宽依赖 窄依赖: 父RDD的分区一一对应到子RDD的分区, 独生 宽依赖: 父RDD的分区被多个子RDD分区使用, 超生 宽窄依赖的不同点: 窄依赖支持统一节点进行链式操作执行多条指令, 无需等待其他父RDD的分区操作; 宽依赖需要所有父分区都是可用的 窄依赖失败恢复更有效, 只需要重新计算丢失的父分区; 宽依赖涉及RDD各级多个父分区 Checkpoint Checkpoint : 检查点, 在计算过程中有一些比较耗时的RDD, 先将其缓存到硬盘或HDFS中, 标记这个RDD有被检查点处理过, 并清空它所有的依赖关系, 同时给它新建一个依赖于 CheckpointRDD的依赖关系, CheckpointRDD可以用来从硬盘中读取RDD和生成新的分区信息. 当某个RDD需要错误恢复时, 追溯到耗时的RDD时, 发现被检查点记录过, 就直接从硬盘中读取该RDD, 无需重新计算 Preferred Location 针对每一个分片, 都会选择一个最优的位置进行计算, 实现计算向数据靠拢 Storage Level 用来记录RDD持久化时存储的级别 MEMORY_ONLY: 只存在缓存中, 如果内存不够则不缓存剩余的部分 默认存储级别 MEMORY_AND_DISK: 缓存在内存中, 如果内存不够则存至硬盘 DISK_ONLY: 只存硬盘 MEMORY_ONLY_2 和 MEMORY_AND_DISK_2等: 与上面的级别和功能相同, 只不过每个分区在集群两个节点上建立副本 Iterator 迭代函数和计算函数, 用来表示如何通过父RDD计算得来的 迭代函数首先会判断缓存中是否有想要计算的 RDD, 如果有就直接读取, 如果没有就查找想要计算的 RDD 是否被检查点处理过. 如果有, 就直接读取, 如果没有, 就调用计算函数向上递归, 查找父 RDD 进行计算 "},"Linux/Hadoop/":{"url":"Linux/Hadoop/","title":"Hadoop","keywords":"","body":"Hadoop "},"Linux/Hadoop/搭建Hadoop.html":{"url":"Linux/Hadoop/搭建Hadoop.html","title":"搭建Hadoop","keywords":"","body":"搭建Hadoop 搭建前准备(Windows需要安装虚拟机) 1. VMware Workstation Pro(最新版) https://www.vmware.com/cn/products/workstation-pro/workstation-pro-evaluation.html https://blog.csdn.net/qq_40950957/article/details/80467513 #博客参考 2. centos镜像文件(centos7-x版本) http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1810.iso 3. jdk (选择7-x以上版本,Linux .tar.gz的文件下载) https://www.oracle.com/technetwork/java/javase/downloads/index.html 4. Hadoop(2.7.0及以上版本) http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz #安装步骤参考博客 https://blog.csdn.net/acecai01/article/details/82696363 开始安装 1. 安装 VMware Workstation Pro 2. 新建虚拟机并配置内存,处理器.硬盘,网络,指定Centos光盘位置 3. 开启虚拟机, 配置时间,分区,网络连接方式,桌面安装找到Desktop安装(纯字符界面使用Minimal安装) 4. 在 VMware Workstation Pro 菜单栏找到虚拟机,安装 VMwareTools Centos中找到 VMwareTools-x.x.0-xx.tar.gz 文件(/run/media目录下)执行 tar -zxvf .tar.gz文件 cd vmware-tools-distrib && ./vmware-install.pl 5. 第一次运行需运行 /usr/bin/vmware-config-tools.pl 命令配置 VMwareTools 6. /usr/bin/vmware-user && startx 7. 将文件从windows桌面拖到Centos桌面 8. 配置免密登陆 cd ~/.ssh/ && ssh-keygen -t rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 9. 安装jdk 查找自带的jdk rpm -qa | grep jdk 卸载自带openjdk yum -y remove java java-xxx-openjdk-xx 将下载的jdk拖到桌面并解压 tar -zxvf jdk-7u80-linux-x64.tar.gz -C /opt 配置环境变量 vim /etc/profile 行首添加 #Set JDK PATH export JAVA_HOME=/opt/jdk1.7.0_80 export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin source /etc/profile update-alternatives --install /usr/bin/java java /opt/jdk1.7.0_80/bin/java 60 update-alternatives --config java 10.关闭防火墙 chkconfig --level 35 iptables off vim /etc/selinux/config #找到SELINUX行修改为：SELINUX=disabled 11. 安装Hadoop http://dblab.xmu.edu.cn/blog/install-hadoop/ #参考文章 将下载的hadoop包拖到桌面并解压缩 tar -zxvf hadoop-2.7.6.tar.gz -C /opt mv /opt/hadoop-2.7.0 /opt/hadoop 配置环境变量 vim /etc/profile 行首添加 #Set HADOOP PATH export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:/opt/hadoop/sbin:/opt/hadoop/bin source /etc/profile gedit /etc/hostname 更改为Master gedit /etc/hosts 最后添加 192.168.121.1 Master 192.168.122.1 Slave1 gedit core-site.xml 添加 hadoop.tmp.dir file:/usr/local/hadoop/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 gedit hdfs-site.xml 添加 dfs.replication 1 dfs.namenode.name.dir file:/usr/local/hadoop/tmp/dfs/name dfs.datanode.data.dir file:/usr/local/hadoop/tmp/dfs/data dfs.namenode.http.address 127.0.0.1:50070 /opt/hadoop/bin/hdfs namenode -format #配置完成后初始化NameNode /opt/hadoop/sbin/start-dfs.sh #开启 NameNode 和 DataNode 守护进程 jps 查看 NameNode 和 DataNode 和 SecondaryNameNode 打开 http://Master:50070 即可查看 NameNode 和 Datanode 信息 12. 运行Hadoop伪分布式实例 运行自带实例: hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+' 运行自己写的python mapreduce程序 hdfs dfs -mkdir -p input/test.txt 本地生成目录及文件 along_mk 其下两个文件 map_wctest.py 和 reduce_wctest.py 1. shell运行 echo 'a b c a b c e a e'| python map_wctest.py | sort -k1,1 | python reduce_wctest.py 2. 上传到hdfs上运行 hdfs dfs -put /root/along_mk along_mk /opt/hadoop/bin/hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.6.jar -files \"map_wctest.py,reduce_wctest.py\" -input input/test.py -output output -mapper \"python /root/along_mr/map_wctest.py\" -reducer \"python /root/along_mr/reduce_wctest.py\" hdfs dfs -cat output/part-00000 查看运行结果 虚拟机配置 along_mk目录下的map和reduce python文件 #map_wctest.py import sys def read_input(file): for line in file: yield line.split() def main(): data = read_input(sys.stdin) for words in data: for word in words: print(f'{word}\\t1') if __name__ == '__main__': main() #reduce_wctest.py import sys from itertools import groupby from operator import itemgetter def read_mapper_output(file, sep='\\t'): for line in file: yield line.rstrip().split(sep, 1) def main(): data = read_mapper_output(sys.stdin) for current_word, group in groupby(data, itemgetter(0)): total_count = sum(int(count) for current_word, count in group) print(f'{current_word}\\t{total_count}') if __name__ == '__main__': main() "},"Linux/Hadoop/Hadoop使用.html":{"url":"Linux/Hadoop/Hadoop使用.html","title":"Hadoop使用","keywords":"","body":"Hadoop使用 服务器与hdfs文件系统进行交互的命令 hdfs dfs | hadoop dfs | hadoop fs 区别与联系 hadoop fs: #可以用于其他文件系统, 不止hdfs文件系统内 (FS relates to a generic file system which can point to any file systems like local, HDFS etc. So this can be used when you are dealing with different file systems such as Local FS, HFTP FS, S3 FS, and others) hadoop dfs: #专门针对hdfs分布式文件系统 (dfs is very specific to HDFS. would work for operation relates to HDFS. This has been deprecated and we should use hdfs dfs instead) hdfs dfs: #与hadoop dfs一样, 但是更为推荐,在使用hadoop dfs 时内部会被转化为hdfs dfs 与hdfs文件系统交互常用指令 -- 很多命令都是 hdfs dfs - #列出hdfs下的文件 hdfs dfs -ls #将Linux系统本地文件上传到HDFS中 hdfs dfs -put #将HDFS中的文件下载到Linux系统本地目录 hdfs dfs -get #查看文件 hdfs dfs -cat #建立目录 hdfs dfs -mkdir #删除文件 hdfs dfs -rm -r #统计文件个数 hdfs dfs -count /user/hive/warehouse/ysx_test.db/along_partition_test/partition_by_month=202001 #统计文件夹下各个文件/文件夹的大小 hdfs dfs -du -h /user/hive/warehouse/ysx_test.db/along_partition_test/ #统计文件夹的大小 hdfs dfs -du -h -s /user/hive/warehouse/ysx_test.db/along_partition_test/ -s: 统计文件夹的大小, 不带-s则为统计文件夹下各个文件/文件夹大小 -h: 以K/M/G为单位展示大小, 不带-h则为字节数 查看yarn任务及终止application进程 #查看 http://:8088/cluster/apps/RUNNING yarn application -list #终止进程 yarn application -kill "},"Linux/Hive/":{"url":"Linux/Hive/","title":"Hive","keywords":"","body":"Docker "},"Linux/Hive/Hive简介.html":{"url":"Linux/Hive/Hive简介.html","title":"Hive简介","keywords":"","body":"Hive简介 什么是 Hive 1: Hive 由 Facebook 实现并开源, 基于Hadoop 的一个数据仓库工具 2: 将结构化的数据映射为一张数据库表并提供HQL(Hive SQL) 查询功能 3: 底层的结构化数据存储在 HDFS 上, 本质是将 HQL 语句转换为 MapReduce 任务运行 Hive 依赖于 HDFS 存储数据, Hive 将 HQL 转换成 MapReduce 执行, 所以说 Hive 是基于 Hadoop 的一个数据仓库工具, 实质就是一款基于 HDFS 的 MapReduce 计算框架, 对存储在 HDFS 中的数据进行分析和管理 为什么使用 Hive 直接使用 MapReduce 所面临的问题: 人员学习成本太高; 项目周期要求太短; MapReduce 实现复杂查询逻辑开发难度太大 Hive的优势: 1: 更友好的接口: 操作接口采用类 SQL 的语法, 提供快速开发的能力 2: 更低的学习成本: 避免了写 MapReduce, 减少开发人员的学习成本 3: 更好的扩展性: 可自由扩展集群规模而无需重启服务; 支持用户自定义函数 Hive的缺点: 1: 不支持数据记录级别的增删改查 (有数据变更可以通过insert overwrite 重新插入数据) 2: 因为MapReduce Job启动过程查询延时严重, 查询延时严重 Hive架构 1 用户接口: Shell/CLI; JDBC/ODBC; WebUI 1: CLI(Command Line Interface)与Shell 终端命令行, 采用交互形式使用 Hive命令行与 Hive进行交互 2: JDBC/ODBC 是Hive基于JDBC操作提供的客户端, 可以使用户通过连接到Hive Server服务 3: WebUI 通过浏览器访问Hive 2 跨语言服务: thrift server Thrift是Facebook开发的软件框架, 用来进行可扩展且跨语言的服务的开发, Hive 集成了该服务,能让不同的编程语言调用 Hive 的接口 3 底层的Driver: Driver(驱动器); Compiler(编译器); Optimizer(优化器); Executor(执行器) Driver 组件完成 HQL 查询语句从词法分析, 语法分析, 编译, 优化, 以及生成逻辑执行 计划的生成; 生成的逻辑执行计划存储在 HDFS 中, 并随后由 MapReduce 调用执行 Hive 的核心是驱动引擎, 驱动引擎由四部分组成： Driver(驱动器): 解释器的作用是将 HiveSQL 语句转换为抽象语法树(AST) Compiler(编译器): 编译器是将语法树编译为逻辑执行计划 Optimizer(优化器): 优化器是对逻辑执行计划进行优化 Executor(执行器): 执行器是调用底层的运行框架执行逻辑执行计划 4 元数据存储系统 元数据就是存储在Hive中的数据的描述信息, 包括表的名字/列/分区/属性(内外部表)/数据所在路径等 Metastore 默认存在自带的 Derby 数据库中 缺点是不适合多用户操作, 并且数据存储目录不固定, 数据库跟着 Hive 走,极度不方便管理, 所以更多的是将元数据保存在自建的MySQL数据库中, Hive和MySQL之间通过Matastore服务交互 执行流程 HiveQL 通过命令行或者客户端提交, 经过 Compiler 编译器, 运用 MetaStore 中的元数据进行类型检测和语法分析, 生成一个逻辑方案 (Logical Plan), 然后通过的优化处理, 产生一个 MapReduce 任务 Hive数据组织 Hive 的存储结构包括数据库,表,视图,分区和表数据等 1 数据库,表,分区等都对应 HDFS 上的一个目录, 表数据对应 HDFS 对应目录下的文件 2 Hive中所有的数据都存储在 HDFS 中, 没有专门的数据存储格式, 支持 TextFile,SequenceFile,RCFiled等自定义格式等, 只要在建表的时候告诉Hive数据中的列分隔符和行分隔符, Hive就可以解析数据 3 数据模型 database: 在 HDFS 中表现为 ${hive.metastore.warehouse.dir} 目录下一个文件夹(通过set hive.metastore.warehouse.dir; 查看默认路径) table: 在 HDFS 中表现所属 database 目录下一个文件夹 external table: 与 table 类似,不过其数据存放位置可以指定任意 HDFS 目录路径 partition: 在 HDFS 中表现为 table 目录下的子目录 bucket: 在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散列之后的多个文件 view: 与传统数据库类似, 只读, 基于基本表创建 内部表(MANAGED_TABLE)和外部表(EXTERNAL_TABLE)的区别 (drop table语句)删除内部表的时候, 删除表元数据和数据 (drop table语句)删除外部表的时候, 删除表元数据, 不删除数据 如果数据的所有处理都在Hive中进行, 倾向于选择内部表; 如果Hive和其他工具针对相同的数据集进行处理, 外部表更合适(自己使用的表建内部表方便操作, 多人使用的建外部表防止误删) 分区表和分桶表的区别 分区表是手动添加分区, 分桶表的数据是按照某些分桶字段进行 hash 散列形成的多个文件, 数据准确性更高 "},"Linux/Hive/Hive使用.html":{"url":"Linux/Hive/Hive使用.html","title":"Hive使用","keywords":"","body":"Hive使用 -- 启动客户端 # hive 和 hive --service cli命令一样 hive hive --service cli --创建表 create [external] table `ysx_test.along_insert_test`( `order_id` bigint, `order_callback_time` string, `partition_by_month` string, `etl_load_at` string) partitioned by(`dt` string) row format delimited fields terminated by '\\u0001' ; [location '/user/hive/warehouse/ysx_test_external/along_insert_test';] 创建表加参数external则指定创建的表为外部表, location不加则为默认的hdfs路径(通过set hive.metastore.warehouse.dir; 查看默认路径) ROW FORMAT DELIMITED: 分隔符设置开始语句 FIELDS TERMINATED BY: 设置字段与字段之间的分隔符 COLLECTION ITEMS TERMINATED BY: 设置一个复杂类型(array,struct)字段的各个item之间的分隔符 MAP KEYS TERMINATED BY: 设置一个复杂类型(Map)字段的key value之间的分隔符 LINES TERMINATED BY: 设置行与行之间的分隔符 #ROW FORMAT DELIMITED 必须在其它分隔设置之前,也就是分隔符设置语句的最前 #LINES TERMINATED BY必须在其它分隔设置之后,也就是分隔符设置语句的最后 -- load语法 load data [local] inpath 'filepath' [overwrite] overwrite覆盖表中原内容 load data local inpath '/root/data/data.txt' overwrite into table partition_table partition(gender='M'); -- 查看表信息 #简要信息 desc ysx_test.along_insert_test; #详细信息 desc extended ysx_test.along_insert_test; #完整信息 desc formatted ysx_test.along_insert_test; #建表语句 show create table ysx_test.along_insert_test; -- 查看Hive参数 set hive属性名; 例如查看集群设置的文件块大小 set dfs.block.size; Hive元数据表 Hive版本的元数据表(VERSION) SCHEMA_VERSION: Hive版本 VERSION_COMMENT: 版本说明 如果该表出现了问题, 根本进入不了Hive-Cli Hive数据库相关的元数据表(DBS, DATABASE_PARAMS) DBS: 存储Hive所有数据库的基本信息 DB_ID: 数据库ID DESC: 数据库描述 DB_LOCATION_URI: 数据库 HDFS 路径 NAME: 数据库名 OWNER_NAME: 数据库所有者用户名 OWNER_TYPE: 所有者角色 DATABASE_PARAMS: 该表存储数据库的相关参数, 在 CREATE DATABASE 时候用WITH DBPROPERTIES (property_name=property_value, …) 指定的参数 DB_ID: 数据库ID PARAM_KEY: 参数名 PARAM_VALUE: 参数值 Hive表和视图相关的元数据表(主要: TBLS, TABLE_PARAMS, TBL_PRIVS这三张表,通过TBL_ID关联 TBLS: 存储Hive表, 视图, 索引表的基本信息 TBL_ID: 表ID CREATE_TIME: 创建时间 DB_ID: 数据库 LAST_ACCESS_TIME: 上次访问时间 OWNER: 所有者 RETENTION: 保留字段 SD_ID: 序列化配置信息 TBL_NAME: 表名 TBL_TYPE: 表类型 VIEW_EXPANDED_TEXT: 视图的详细 HQL VIEW_ORIGINAL_TEXT: 视图的原始 HQL TABLE_PARAMS: 存储表/视图的属性信息 TBL_ID: 表ID PARAM_KEY: 属性名 PARAM_VALUE: 属性值 TBL_PRIVS: 存储表/视图的授权信息 TBL_GRANT_ID: 授权ID CREATE_TIME: 授权时间 GRANT_OPTION: GRANTOR: 授权执行用户 GRANTOR_TYPE: 授权者类型 PRINCIPAL_NAME: 被授权用户 PRINCIPAL_TYPE: 被授权用户类型 TBL_PRIV: 权限 TBL_ID: 表ID Hive 文件存储信息相关的元数据表(主要: SDS, SD_PARAMS, SERDES, SERDE_PARAMS) 由于 HDFS 支持的文件格式很多, 而建 Hive 表时候也可以指定各种文件格式, Hive 在将 HQL 解析成 MapReduce 时候, 需要知道去哪里, 使用哪种格式去读写 HDFS 文件, 而这些信息就保存在这几张表中 SDS: 该表保存文件存储的基本信息, 如 INPUT_FORMAT, OUTPUT_FORMAT, 是否压缩等, TBLS 表中的 SD_ID 与该表关联, 可以获取 Hive 表的存储信息 SD_ID: 存储信息ID CD_ID: 字段信息ID INPUT_FORMAT: 文件输入格式 IS_COMPRESSED: 是否压缩 IS_STOREDASSUBDIRECTORIES: 是否以子目录存储 LOCATION: HDFS NUM_BUCKETS: 分桶数量 OUTPUT_FORMAT: 文件输出格式 SERDE_ID: 序列化类ID SD_PARAMS: 该表存储 Hive 存储的属性信息，在创建表时候使用STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (…) 指定 SD_ID: 序列化类配置ID PARAM_KEY: 存储属性名 PARAM_VALUE: 存储属性值 SERDES: 该表存储序列化使用的类信息 SD_ID: 序列化类配置ID NAME: 序列化类别名 SLIB: 序列化类 SERDE_PARAMS: 该表存储序列化的一些属性的格式信息, 比如: 行, 列分隔符 SD_ID: 序列化类配置ID PARAM_KEY: 存储属性名 PARAM_VALUE: 存储属性值 Hive 表字段相关的元数据表(主要: COLUMNS_V2) COLUMNS_V2: 该表存储表对应的字段信息 CD_ID: 字段信息ID(通过SDS表的CD_ID关联对应的Hive表) COMMENT: 字段注释 COLUMN_NAME: 字段名 TYPE_NAME: 字段类型 INTEGER_IDX: 字段顺序 Hive 表分区相关的元数据表(主要: PARTITIONS, PARTITION_KEYS, PARTITION_KEY_VALS, PARTITION_PARAMS) PARTITIONS: 该表存储表分区的基本信息 PART_ID: 分区ID CREATE_TIME: 分区创建时间 LAST_ACCESS_TIME: 最后一次访问时间 PART_NAME: 分区名 SD_ID: 分区存储ID TBL_ID: 表ID PARTITION_KEYS: 该表存储分区的字段信息 TBL_ID: 表 PKEY_COMMENT: 分区字段说明 PKEY_NAME: 分区字段名 PKEY_TYPE: 分区字段类型 INTEGER_IDX: 分区字段顺序 PARTITION_KEY_VALS: 该表存储分区字段值 PART_ID: 分区ID PART_KEY_VAL: 分区字段值 PART_KEY_IDX: 分区字段值顺序 PARTITION_PARAMS: 该表存储分区的属性信息 PART_ID: 分区ID PARAM_KEY: 分区属性名 PARAM_VALUE: 分区属性值 其他不常用的表 DB_PRIVS: 数据库权限信息表,通过 GRANT 语句对数据库授权后, 将会在这里存储 IDXS: 索引表, 存储 Hive 索引相关的元数据 INDEX_PARAMS: 索引相关的属性信息 TAB_COL_STATS:表字段的统计信息,使用 ANALYZE 语句对表字段分析后记录在这里 TBL_COL_PRIVS: 表字段的授权信息 PART_PRIVS: 分区的授权信息 PART_COL_STATS: 分区字段的统计信息 PART_COL_PRIVS: 分区字段的权限信息 FUNCS: 用户注册的函数信息 FUNC_RU: 用户注册函数的资源信息 "},"Linux/Hive/Hive参数.html":{"url":"Linux/Hive/Hive参数.html","title":"Hive参数","keywords":"","body":"Hive参数 #查看集群设置的文件块大小, 默认128M, 只能在配置文件修改,不能在hive中自定义修改 set dfs.block.size; #是否动态分区 set hive.exec.dynamici.partition --默认false:未开启动态分区;true:开启 #动态分区是否允许所有的分区列都是动态分区列 set hive.exec.dynamic.partition.mode --默认strict:不允许所有分区列全部是动态的; nostrick:允许 #每个mapreduce job允许创建的分区的最大数量 set hive.exec.max.dynamic.partitions.pernode --默认100 #一个dml语句允许创建的所以分区的最大数量 set hive.exec.max.dynamic.partitions --默认1000 #所有的mapreduce job允许创建的文件的最大数量 set hive.exec.max.created.files --默认100000 #当输出文件的平均大小小于该值时,启动一个独立的map-reduce任务进行文件merge set hive.merge.smallfiles.avgsize --例如16777216为16M "},"Linux/Hive/Hive数据倾斜问题.html":{"url":"Linux/Hive/Hive数据倾斜问题.html","title":"Hive数据倾斜问题","keywords":"","body":"Hive数据倾斜问题 数据倾斜, 即单个节点任务所处理的数据量远大于同类型任务所处理的数据量, 导致该节点成为整个作业的瓶颈, 这是分布式系统不可能避免的问题. 从本质上说, 导致数据倾斜有两种原因: 1 任务读取大文件, 2 任务需要处理大量相同键的数据 mapreduce实际处理流程 input --> map --> partition --> combine --> sort --> reduce --> output 任务读取大文件 最常见的是读取压缩的不可分割的大文件, 当对数据进行归档或转储时会需要对数据进行压缩, 当对文件使用gzip压缩等不支持文件分割的压缩方式时, 读取该压缩文件只会被一个map任务读取, 如果压缩文件很大, 处理该文件的map需要花费的时间会远大于读取普通文件的map时间, 该map任务就会成为作业运行的瓶颈 解决方案: 1) 数据压缩时采用bzip2/zip/snappy等压缩算法 2) 合理分区 任务需要处理大量相同键的数据 1 业务无关的数据/异常值引发的数据倾斜 实际业务中有大量的NULL值或者一些无意义的数据参与到计算作业中, 这些作业可能来自业务上报/数据规范对数据进行归一化变为NULL/空字符串等, 这些与业务无关的数据引入导致在进行分组聚合或者表连接时发生数据倾斜 解决方案: 1) 根据业务需求排除异常数据; 2) 将异常值与正常值分开处理; 3) 表关联时类型保持一致 2 聚合函数引起的数据倾斜 在mapreduce任务中, reduce接收的数据是分区的结果, 分区是对key求hash值,根据hash值决定key被分到哪个分区,进而被分配到某个reduce. 在聚合函数的过程中, 如果聚合的维度值很集中,那分配到某些reduce任务的数据量就会很大, 该reduce任务处理的数据量将远大于其他reduce任务, 该reduce任务就会成为作业运行的瓶颈 解决方案: 1) 在map阶段给key增加一个随机数, 减小被分配到同一个reduce的概率 2) 设置负载均衡 hive.groupby.skewindata=true, 开启后会有两个mapreduce任务, 第一个job中, map阶段的输出结果会随机分布到reduce中,每个reduce做部分聚合操作然后输出给第二个mapreduce任务,第二个job会根据key的hash值进行分配,完成最终的聚合. 核心原理就是第一个任务随机分配到不同的reduce上进行计算做部分聚合, 减少数据量, 然后第二个任务进行最终聚合 3) map端开启聚合, 减少进入到reduce阶段的数据量 set hive.map.aggr=true; 4) 设置reduce能处理的数据大小 set hive.exec.reducers.bytes.per.reducer=1000000000 (单位为字节,默认1G) 5) 设置最大开启的reduce个数 set hive.exec.reducers.max=999 (默认999) 与4同时配置都不会生效 6) 增加资源, 增加jvm内存 7) 将count(distinct) 改成 sum...group by的形式 select a,count(distinct b) from t group by a --> select a,sum(1) from (select a, b from t group by a,b) group by a "},"Linux/Docker/":{"url":"Linux/Docker/","title":"Docker","keywords":"","body":"Docker "},"Linux/Docker/Docker.html":{"url":"Linux/Docker/Docker.html","title":"Docker","keywords":"","body":"Docker Docker概念 Docker--从入门到实践 Docker配置Mysql 1. docker pull mysql:latest #拉取最新版mysql 2. docker images #查看镜像文件 3. docker run -p 3306:3306 --name along_mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql #启动mysql服务并映射端口号为3306,启动name为along_mysql,设置初始密码为123456 4. docker ps -a #查看启动的服务,如果上一步报错,有可能是关闭了docker容器但是还未退出,STATUS为Exited 如果发现有关闭了容器但是还未退出的容器,使用 docker rm 删除 --批量删除 docker container prune --批量删除 docker ps -a | sed '/^CONTAINER/d' | grep \"Exited\" | gawk '{cmd=\"docker rm \"$1; system(cmd)}' 5. 不建议直接进入docker容器内部修改内容,如果需要修改密码/CURD数据,使用宿主机/外部访问宿主机ip:3306端口操作!!! ##宿主机/外部访问docker启动的mysql服务 1. 宿主机上安装mysql yum install mysql-community-server service mysqld start #停止service mysqld stop service mysqld status #查看状态 #如果centos上安装mariadbs数据库 删除yum remove mariadb* 2. 停止docker上的mysql服务 docker stop mysql / docker rm #因为宿主机上启动mysql服务也是用3306端口,如果不中断docker的mysql服务,宿主机上无法启动mysql并报错 3. 宿主机上登陆mysql cat /var/log/mysqld.log | grep password --获取安装时的用户密码,假设为-dz%-KLw0fe mysql -uroot -p -dz%-KLw0fe ALTER USER \"root\"@\"localhost\" IDENTIFIED BY \"123456\"; #修改密码 #mysql8.*+在修改密码时会报错 \"Your password does not satisfy the current policy requirements\" ALTER USER \"root\"@\"localhost\" IDENTIFIED BY \"Root_123456\"; 先修改为指定强度的密码,然后修改密码强度 set global validate_password.policy=0; set global validate_password.length=1; ALTER USER \"root\"@\"localhost\" IDENTIFIED BY \"123456\"; #修改为简单密码 4. 宿主机上mysql服务中断, service mysqld stop, 启动docker上mysql服务 5. 设置外部访问,如不设置,脱离虚拟机使用其他工具如Navicat/Mysql-Front连接会报错\"1251-client does not support authentication protocol requested by server\" 宿主机连接docker服务 mysql -h192.168.112.131 -uroot -p123456 #192.168.112.131为你当前虚拟机的IP地址 使用ifconfig查看到inet select host,user,plugin,authentication_string from mysql.user; #查看用户信息 update user set host = '%' where user ='root'; #host不设限,都可访问 flush privileges; ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456' 6. 使用Navicat 连接 192.168.112.131 root 123456 Docker部署mindoc(wiki) 1. docker pull registry.cn-hangzhou.aliyuncs.com/mindoc/mindoc:v0.12 #使用阿里云拉取mindoc docker镜像 2. mysql -h192.168.112.131 -uroot -p123456 #创建wiki所需数据库 (mysql)>CREATE DATABASE mindoc_db 3. docker run -p 8181:8181 -e MYSQL_PORT_3306_TCP_ADDR=192.168.112.131 -e MYSQL_PORT_3306_TCP_PORT=3306 -e MYSQL_INSTANCE_NAME=mindoc_db -e MYSQL_USERNAME=root -e MYSQL_PASSWORD=123456 -e httpport=8181 -d registry.cn-hangzhou.aliyuncs.com/mindoc/mindoc:v0.12 #启动mindoc DB_ADAPTER 制定 DB MYSQL_PORT_3306_TCP_ADDR MySQL地址 MYSQL_PORT_3306_TCP_PORT MySQL端口号 MYSQL_INSTANCE_NAME MySQL数据库名称 MYSQL_USERNAME MySQL账号 MYSQL_PASSWORD MySQL密码 HTTP_PORT 程序监听的端口号 4. 访问 192.168.112.131:8181 "},"Vue/":{"url":"Vue/","title":"Vue","keywords":"","body":"Vue "},"Vue/vue模版语法.html":{"url":"Vue/vue模版语法.html","title":"vue模板语法","keywords":"","body":"vue模板语法 模版语法 https://vuejs.bootcss.com/guide/syntax.html 1: 插值 文本: 双大括号 {{ number }} html: v-html 属性: v-bind:key='value' 表达式: {{ JavaScript表达式 }} 只能使用单个表达式,例: {{ number + 1 }}, {{ value === 'ok' ? 'yes' : 'no' }} 错误例: {{ var a = 1 }}, {{ if (value === 'ok') { return message } }} 2: 指令 指令是带有 v- 前缀的属性,指令必须添加到元素上,单个使用指令无意义也不生效 指令的作用是在 表达式的值发生改变时,产生连带影响作用于DOM,改变样式 例如: v-if 现在你看到我了 v-if指令通过表达式seen的值来插入/移除 元素 带参数的指令: 一些指令通过接受参数来更新html的属性 例如: v-bind ... v-bind后跟的是元素 的属性, 代表的意思是 将元素的 href属性与表达式url的值绑定 v-on ... v-on用来监听DOM事件,比如点击事件,输入事件 这里代表的意思是将 的点击事件与函数doSomething绑定 动态参数: 用方括号括起来的 JavaScript表达式作为指令的参数,例如: ... ... attributeName = key attributeName = key2 这里的attributeName会被作为 JavaScript表达式进行动态求值,得到的结果当作参数与url的值进行绑定 v-on的动态参数绑定: ... 注: 在 html元素中避免使用大写字符来命名键名,因为浏览器会把所有的属性名全部小写, 例如 ... 这段代码会被转成 v-bind:[someattr], 如果未定义 someattr则无法识别 3: 缩写 v- 前缀是有独特的标识意义,但是在一些经常用到的指令,写起来就比较繁琐, 所以vue对两个最常用的属性提供了特殊简写 v-bind 缩写 ... ... ... v-on 缩写 ... ... ... 条件渲染 v-if v-if指令用于条件渲染一块内容, 改指令只会在表达式返回truthy值的时候被渲染, 例如 Vue is awesome! 只会在awesome的值为真值的时候被渲染, js的真值判断 v-if添加到元素上可以用来控制切换多个元素, 在上使用v-if,最终的渲染结果不包含 Title Paragraph 1 Paragraph 2 v-else/v-else-if v-else: 必须跟在 v-if/ v-else-if后面,否则不会被识别 0.5\">Now you see me Now you do not v-else-if: v-if的多条件补充, 必须跟在 v-if/ v-else-if后面 v-show Hello! 带有v-show的元素会始终被渲染保存在DOM中, v-show只是切换元素的CSS属性 display, v-show不支持 元素, 也不支持 v-else 比较: v-if 是真正的条件渲染, 它会确保在切换过程中条件块内的子组建销毁和重建; v-show不论什么条件都会进行渲染,只是基于CSS的切换 v-if有更高的切换开销, v-show有更高的初始渲染开销 建议: 频繁切换用v-show, 运行条件很少改变用v-if 列表渲染 v-for arr: v-for指令基于一个数组来渲染一个列表, v-for指令使用形式如 item in items (item of items)的语法, items是元数据数组, item是被迭代的数据的别名 items:[{ message: 'Foo' },{ message: 'Bar' }] {{ item.message }} 结果: Foo Bar v-for第二个参数, 为当前项的索引 :key=\"item.message\"> {{ index }} - {{ item.message }} 结果: 0-Foo 1-Bar object: v-for遍历一个对象的属性 object: { title: 'How to do lists in Vue', author: 'Jane Doe', publishedAt: '2016-04-10' } {{ value }} 结果: How to do lists in Vue Jane Doe 2016-04-10 v-for第二个参数访问对象的键名 {{ name }}: {{ value }} 结果: title:How to do lists in Vue author:Jane Doe publishedAt:2016-04-10 为了给vue一个提示,以便跟踪到每个节点的身份,从而重用和重拍现有元素,在进行v-for渲染时, 需要为对应的元素提供一个 唯一的 key 属性 对于arr来说, 唯一key是arr的索引,例: 对于object来说, 唯一key是键, 例: 不推荐在同一元素上使用v-if和v-for: 当它们处于同一节点时, v-for的优先级比v-if更高, 意味着, v-if将分别重复运行于每个v-for循环中 生命周期 : https://vuejs.bootcss.com/guide/instance.html#生命周期图示 "},"Vue/vue响应式原理.html":{"url":"Vue/vue响应式原理.html","title":"vue响应式原理","keywords":"","body":"vue响应式原理 官网响应式原理:https://vuejs.bootcss.com/guide/reactivity.html 详解响应式原理:https://www.cnblogs.com/fundebug/p/responsive-vue.html vue最独特的特性之一是: 非侵入性的响应式系统 vue的数据模型仅是普通的JavaScript对象, 当修改数据模型时, 视图会进行更新 非侵入性 当增加功能时, 代码是否需要使用新的 语法/数据类型/数据结构 前端举例: 使用jQuery时, 表面上是在写JavaScript, 实际上是在写基于 jQuery的API, 为了实现MVVM的效果, 需要创建Model(数据)和View(视图)实例, 为了使数据变化时视图随着更改, 需要操控视图实例(修改DOM); 与之对比的vue只是在给某个数据结构(对象)的某个属性赋值而已, 并不会修改视图实例 后端举例: 实现某个功能的时候, 类A需要继承第三方类库B(侵入) 与 在A类中直接调用类B的某个方法(非侵入) 如何追踪变化 1 当把JavaScript对象传入Vue实例作为data选项, Vue会遍历此对象所有的property, 并使用Object.defineProperty把这些property全部转换为 getter/setter, Object.defineProperty只有在IE8以上版本浏览器才支持, 即Vue也只能在IE8以上的浏览器版本才能使用 ----侦测数据变化(数据劫持/数据代理) 2 每个组件实例都对应一个watcher实例, watcher会在组件rander函数渲染的过程中把接触(\"Touch\")过的对象全部记录为依赖(Dependency), 之后当依赖项的setter触发时, 都会 通知(Notify)到watcher ----收集视图依赖的数据(依赖收集) 3 watcher接收到依赖数据变化后会通知rander函数重新渲染生成虚拟DOM树 ----更新视图(发布订阅模式) 追踪变化的注意事项 由于JavaScript的限制, vue不能检测数组和对象的变化,但是可以通过一些办法来回避这些限制以保证响应性 数组 1 直接利用索引设置一个数组时 2 修改数组长度时 解决办法: 1 使用Vue.set(object, propertyName, value)方法添加 -- this.$set(object, propertyName, value) 是Vue.set的别名 2 使用splice方法更改数组数据 vm.message_list.splice(indexOfitem,1,newValue) 3 使用splice方法更改数组长度 vm.message_list.splice(newLength) 对象 vue无法检测property的添加/移除, vue只会在初始化实例时对property执行getter/setter转化, 所以property必须在data对象上存在才能把它转换为响应式的 解决办法: 1 在初始化的时候添加在data对象上 2 使用Vue.set(object, propertyName, value)方法添加 -- this.$set(object, propertyName, value) 是Vue.set的别名 当为已有对象赋值多个property时, 创建一个新的对象覆盖原对象达到更新的目的 this.someObject = Object.assign({}, this.someObject, newObject) 演示 {{item}} \"message_obj: \"{{message_obj}} \"message_obj.a: \"{{message_obj.a}} 按钮 var vm = new Vue({ el: '#example', data: { message_list: ['a1', 'b2', 'c3', 'd4'], message_obj: {'a': ['a1', 'b2', 'c3', 'd4']}, //在初始化时赋值 // message_obj: {'a': ['a1', 'b2', 'c3', 'd4'], 'b':undefined}, }, methods: { btnclick() { // 数组非响应式更新 vm.message_list[1] = 999; // 直接使用索引更改数组的值 vm.message_list.length = 1; // 直接更改数组的长度 console.log('触发按钮，message_list:', vm.message_list); console.log('触发按钮，message_list长度:', vm.message_list.length); //arr解决方案start // Vue.set(vm.message_list, 1, 999); // this.$set(vm.message_list, 1, 999); // vm.message_list.splice(1,1,999); // vm.message_list.splice(1) // 解决方案: 1 Vue.set(vm.message_list, 1, 999) | this.$set(vm.message_list, 1, 999) // 2 使用splice方法更改数组数据 vm.message_list.splice(1,1,999) // 3 使用splice方法更改数组长度 vm.message_list.splice(1) //arr解决方案end // 对象非响应式更新 vm.message_obj.b = 'b'; // 初始化未定义在data对象中, 非响应式 //object解决方案start // Vue.set(vm.message_obj, 'b', 'b'); // this.$set(vm.message_obj, 'b', 'b'); // this.message_obj = Object.assign({}, this.message_obj, {'c':'c'}, {'d':'d'}); // 解决方案: 1 在初始化时赋值, message_obj: {'a': ['a1', 'b2', 'c3', 'd4'], 'b':undefined} // 2 Vue.set(vm.message_obj, 'b', 'b') | this.$set(vm.message_obj, 'b', 'b') // 3 创建新的对象 this.message_obj = Object.assign({}, this.message_obj, {'c':'c'}, {'d':'d'}) //object解决方案end } }, }); "},"QuestionBank/":{"url":"QuestionBank/","title":"QuestionBank","keywords":"","body":"Introduction 定期选择力扣题目，共同研究学习，提高代码掌控力和逻辑思维能力，更好更快的写出优雅贵气的代码 "},"QuestionBank/20190807.html":{"url":"QuestionBank/20190807.html","title":"20190807","keywords":"","body":"from leetcode： Q:给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： 输入: \"babad\" 输出: \"bab\" 注意: \"aba\" 也是一个有效答案。 示例 2： 输入: \"cbbd\" 输出: \"bb A: class Solution: def longestPalindrome(self, s: str) -> str: n = len(s) dp = [[0] * n for _ in range(n)] max_len = float(\"-inf\") res = \"\" for i in range(n): for j in range(i, -1, -1): if s[i] == s[j] and (i - j max_len: max_len = i - j + 1 res = s[j:i + 1] return res "}}